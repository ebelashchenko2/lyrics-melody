{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "train_decoupled_performer.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qux-zfGDXrVk",
        "outputId": "af612ca9-3da6-4404-aace-649c17ab9282"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!pip install drive/MyDrive/lmd_transformer/pytorch_fast_transformers-0.3.0-cp37-cp37m-linux_x86_64.whl\n",
        "!pip install deepspeed==0.3.10\n",
        "!pip install transformers\n",
        "!git clone https://github.com/gulnazaki/performer-pytorch.git\n",
        "!pip install ./performer-pytorch"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "Processing ./drive/MyDrive/lmd_transformer/pytorch_fast_transformers-0.3.0-cp37-cp37m-linux_x86_64.whl\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from pytorch-fast-transformers==0.3.0) (1.7.1+cu101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch->pytorch-fast-transformers==0.3.0) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->pytorch-fast-transformers==0.3.0) (3.7.4.3)\n",
            "Installing collected packages: pytorch-fast-transformers\n",
            "Successfully installed pytorch-fast-transformers-0.3.0\n",
            "Collecting deepspeed==0.3.10\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3f/bd/b2b544ca1286252e9a559b1508e64d0d61af7a73b6bf6737568858128e11/deepspeed-0.3.10.tar.gz (281kB)\n",
            "\u001b[K     |████████████████████████████████| 286kB 20.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=1.2 in /usr/local/lib/python3.7/dist-packages (from deepspeed==0.3.10) (1.7.1+cu101)\n",
            "Requirement already satisfied: torchvision>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from deepspeed==0.3.10) (0.8.2+cu101)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from deepspeed==0.3.10) (4.41.1)\n",
            "Collecting tensorboardX==1.8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c3/12/dcaf67e1312475b26db9e45e7bb6f32b540671a9ee120b3a72d9e09bc517/tensorboardX-1.8-py2.py3-none-any.whl (216kB)\n",
            "\u001b[K     |████████████████████████████████| 225kB 48.3MB/s \n",
            "\u001b[?25hCollecting ninja\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1d/de/393468f2a37fc2c1dc3a06afc37775e27fde2d16845424141d4da62c686d/ninja-1.10.0.post2-py3-none-manylinux1_x86_64.whl (107kB)\n",
            "\u001b[K     |████████████████████████████████| 112kB 36.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from deepspeed==0.3.10) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.2->deepspeed==0.3.10) (3.7.4.3)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision>=0.4.0->deepspeed==0.3.10) (7.0.0)\n",
            "Requirement already satisfied: protobuf>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from tensorboardX==1.8->deepspeed==0.3.10) (3.12.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from tensorboardX==1.8->deepspeed==0.3.10) (1.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.2.0->tensorboardX==1.8->deepspeed==0.3.10) (53.0.0)\n",
            "Building wheels for collected packages: deepspeed\n",
            "  Building wheel for deepspeed (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for deepspeed: filename=deepspeed-0.3.10-cp37-none-any.whl size=272625 sha256=23906b5faa2dc4b9dff699d0706249fa5ae57ae8e8ba9dbc10a8235c85d59fd5\n",
            "  Stored in directory: /root/.cache/pip/wheels/a3/3c/9c/39a16330874a2c55f61fe2c501e120258975d509177ffdcda7\n",
            "Successfully built deepspeed\n",
            "Installing collected packages: tensorboardX, ninja, deepspeed\n",
            "Successfully installed deepspeed-0.3.10 ninja-1.10.0.post2 tensorboardX-1.8\n",
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f9/54/5ca07ec9569d2f232f3166de5457b63943882f7950ddfcc887732fc7fb23/transformers-4.3.3-py3-none-any.whl (1.9MB)\n",
            "\u001b[K     |████████████████████████████████| 1.9MB 17.5MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 47.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/23/2ddc317b2121117bf34dd00f5b0de194158f2a44ee2bf5e47c7166878a97/tokenizers-0.10.1-cp37-cp37m-manylinux2010_x86_64.whl (3.2MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 45.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.0)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp37-none-any.whl size=893262 sha256=86684b6a29853c8d4c762ae5334d031579803d62ce62997938f95a7e35e89052\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sacremoses, tokenizers, transformers\n",
            "Successfully installed sacremoses-0.0.43 tokenizers-0.10.1 transformers-4.3.3\n",
            "Cloning into 'performer-pytorch'...\n",
            "remote: Enumerating objects: 87, done.\u001b[K\n",
            "remote: Counting objects: 100% (87/87), done.\u001b[K\n",
            "remote: Compressing objects: 100% (56/56), done.\u001b[K\n",
            "remote: Total 507 (delta 52), reused 55 (delta 27), pack-reused 420\u001b[K\n",
            "Receiving objects: 100% (507/507), 35.02 MiB | 54.66 MiB/s, done.\n",
            "Resolving deltas: 100% (335/335), done.\n",
            "Processing ./performer-pytorch\n",
            "Collecting einops>=0.3\n",
            "  Downloading https://files.pythonhosted.org/packages/5d/a0/9935e030634bf60ecd572c775f64ace82ceddf2f504a5fd3902438f07090/einops-0.3.0-py2.py3-none-any.whl\n",
            "Collecting local-attention>=1.1.1\n",
            "  Downloading https://files.pythonhosted.org/packages/a0/86/f1df73868c1c433a9184d94e86cdd970951ecf14d8b556b41302febb9a12/local_attention-1.2.2-py3-none-any.whl\n",
            "Requirement already satisfied: pytorch-fast-transformers>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from performer-pytorch==0.15.0) (0.3.0)\n",
            "Requirement already satisfied: torch>=1.6 in /usr/local/lib/python3.7/dist-packages (from performer-pytorch==0.15.0) (1.7.1+cu101)\n",
            "Collecting axial-positional-embedding>=0.1.0\n",
            "  Downloading https://files.pythonhosted.org/packages/7a/27/ad886f872b15153905d957a70670efe7521a07c70d324ff224f998e52492/axial_positional_embedding-0.2.1.tar.gz\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch>=1.6->performer-pytorch==0.15.0) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.6->performer-pytorch==0.15.0) (3.7.4.3)\n",
            "Building wheels for collected packages: performer-pytorch, axial-positional-embedding\n",
            "  Building wheel for performer-pytorch (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for performer-pytorch: filename=performer_pytorch-0.15.0-cp37-none-any.whl size=12420 sha256=8170587fe8a76c5cf16f0631a86b0a95fa8fe2e2e596c9ea051a275cbb5ebd4d\n",
            "  Stored in directory: /root/.cache/pip/wheels/0d/73/93/041f7dd55e6f33ef90455a36e217ed2811faeb9dd9fe343159\n",
            "  Building wheel for axial-positional-embedding (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for axial-positional-embedding: filename=axial_positional_embedding-0.2.1-cp37-none-any.whl size=2905 sha256=7fae39ee05d9b561bcee8513b86d8b18eb9c4c1478ad89991e6fbd36212e708a\n",
            "  Stored in directory: /root/.cache/pip/wheels/cd/f8/93/25b60e319a481e8f324dcb1871aff818eb0c8143ed20b732b4\n",
            "Successfully built performer-pytorch axial-positional-embedding\n",
            "Installing collected packages: einops, local-attention, axial-positional-embedding, performer-pytorch\n",
            "Successfully installed axial-positional-embedding-0.2.1 einops-0.3.0 local-attention-1.2.2 performer-pytorch-0.15.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qsro2T9RYg8D",
        "outputId": "55e9cc23-14e9-4b62-ef89-f0511a2bbdfe"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sun Feb 28 23:48:05 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.39       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   39C    P8    10W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n7hDc0twYk2y",
        "outputId": "b52beb10-4dcf-4243-c27d-0dd51fae7810"
      },
      "source": [
        "%%writefile ds_config.json\n",
        "\n",
        "{\n",
        "  \"train_batch_size\": 8,\n",
        "  \"gradient_accumulation_steps\": 8,\n",
        "  \"steps_per_print\": 20,\n",
        "  \"gradient_clipping\": 0.5,\n",
        "  \"optimizer\": {\n",
        "    \"type\": \"Adam\",\n",
        "    \"params\": {\n",
        "      \"lr\": 0.001,\n",
        "      \"betas\": [\n",
        "        0.9,\n",
        "        0.98\n",
        "      ],\n",
        "      \"eps\": 1e-8,\n",
        "      \"weight_decay\" : 0.1\n",
        "    }\n",
        "  },\n",
        "  \"scheduler\": {\n",
        "    \"type\": \"WarmupLR\",\n",
        "    \"params\": {\n",
        "      \"warmup_min_lr\": 0,\n",
        "      \"warmup_max_lr\": 0.001,\n",
        "      \"warmup_num_steps\": 100\n",
        "    }\n",
        "  }\n",
        "}"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing ds_config.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RF38wBLZY3hn",
        "outputId": "11f4a472-1e45-4c02-ba16-478e04f64105"
      },
      "source": [
        "%%writefile decoupled_performer.py\n",
        "\n",
        "import re\n",
        "import torch\n",
        "from torch import nn\n",
        "from performer_pytorch.performer_pytorch import PerformerLM\n",
        "from performer_pytorch.autoregressive_wrapper import AutoregressiveWrapper\n",
        "\n",
        "ENC_PREFIX = 'enc_'\n",
        "LM_PREFIX = 'lm_'\n",
        "DEC_PREFIX = 'dec_'\n",
        "\n",
        "def group_dict_by_key(cond, d):\n",
        "    return_val = [dict(),dict()]\n",
        "    for key in d.keys():\n",
        "        match = bool(cond(key))\n",
        "        ind = int(not match)\n",
        "        return_val[ind][key] = d[key]\n",
        "    return (*return_val,)\n",
        "\n",
        "def string_begins_with(prefix, str):\n",
        "    return bool(re.match(f'^{prefix}', str))\n",
        "\n",
        "def group_by_key_prefix(prefix, d):\n",
        "    return group_dict_by_key(lambda x: string_begins_with(prefix, x), d)\n",
        "\n",
        "def group_by_key_prefix_and_remove_prefix(prefix, d):\n",
        "    kwargs_with_prefix, kwargs = group_dict_by_key(lambda x: string_begins_with(prefix, x), d)\n",
        "    kwargs_without_prefix = dict(map(lambda x: (x[0][len(prefix):], x[1]), tuple(kwargs_with_prefix.items())))\n",
        "    return kwargs_without_prefix, kwargs\n",
        "\n",
        "def extract_enc_lm_dec_kwargs(kwargs):\n",
        "    enc_kwargs, kwargs = group_by_key_prefix_and_remove_prefix(ENC_PREFIX, kwargs)\n",
        "    lm_kwargs, kwargs = group_by_key_prefix_and_remove_prefix(LM_PREFIX, kwargs)\n",
        "    dec_kwargs, kwargs = group_by_key_prefix_and_remove_prefix(DEC_PREFIX, kwargs)\n",
        "    return enc_kwargs, lm_kwargs, dec_kwargs, kwargs\n",
        "\n",
        "def extract_and_set_enc_lm_dec_kwargs(kwargs):\n",
        "    enc_kwargs, lm_kwargs, dec_kwargs, kwargs = extract_enc_lm_dec_kwargs(kwargs)\n",
        "    if 'mask' in enc_kwargs:\n",
        "        dec_kwargs.setdefault('context_mask', enc_kwargs['mask'])\n",
        "    if 'mask' in lm_kwargs:\n",
        "        dec_kwargs.setdefault('second_context_mask', lm_kwargs['mask'][:, :-1])\n",
        "    return enc_kwargs, lm_kwargs, dec_kwargs, kwargs\n",
        "\n",
        "class DecoupledPerformer(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        dim,\n",
        "        tie_token_embeds = False,\n",
        "        no_projection = False,\n",
        "        pretrained_lm = \"\",\n",
        "        **kwargs\n",
        "    ):\n",
        "        super().__init__()\n",
        "        enc_kwargs, lm_kwargs, dec_kwargs, _ = extract_enc_lm_dec_kwargs(kwargs)\n",
        "        \n",
        "        assert 'dim' not in enc_kwargs and 'dim' not in dec_kwargs and 'dim' not in lm_kwargs\n",
        "\n",
        "        enc_kwargs['dim'] = lm_kwargs['dim'] = dec_kwargs['dim'] = dim\n",
        "        enc_kwargs['no_projection'] = lm_kwargs['no_projection'] = dec_kwargs['no_projection'] = no_projection\n",
        "\n",
        "        lm_kwargs['causal'] = True\n",
        "        # cross attention has to be set explicitly\n",
        "        if not 'cross_attend' in lm_kwargs:\n",
        "            lm_kwargs['cross_attend'] = False\n",
        "        \n",
        "        self.lm_cross_attending = lm_kwargs['cross_attend']\n",
        "\n",
        "        dec_kwargs['causal'] = True\n",
        "        dec_kwargs['cross_attend'] = True\n",
        "        dec_kwargs['second_cross_attend'] = True\n",
        "\n",
        "        enc = PerformerLM(**enc_kwargs)\n",
        "        lm = PerformerLM(**lm_kwargs)\n",
        "        dec = PerformerLM(**dec_kwargs)\n",
        "\n",
        "        if tie_token_embeds:\n",
        "            enc.token_emb = lm.token_emb = dec.token_emb\n",
        "\n",
        "        self.enc = enc\n",
        "        if pretrained_lm:\n",
        "            pretrained = torch.load(pretrained_lm)\n",
        "            from collections import OrderedDict\n",
        "            new_pretrained = OrderedDict()\n",
        "            if lm_kwargs['reversible']:\n",
        "                if lm_kwargs['cross_attend']:\n",
        "                    for k, v in pretrained.items():\n",
        "                        if len(k.split('.')) >= 5:\n",
        "                            new_pretrained['performer.net.blocks.{}.{}'.format(int(k.split('.')[3])*2, k.split('.', 4)[-1])] = pretrained[k]\n",
        "                        else:\n",
        "                            new_pretrained[k] = pretrained[k]\n",
        "                else:\n",
        "                    new_pretrained = pretrained\n",
        "            else:\n",
        "                for k, v in pretrained.items():\n",
        "                    if len(k.split('.')) >= 5 and k.split('.')[4] == 'f':\n",
        "                        new_pretrained['performer.net.layers.{}.0.{}'.format(k.split('.')[3], k.split('.', 6)[-1])] = pretrained[k]\n",
        "                    elif len(k.split('.')) >= 5 and k.split('.')[4] == 'g':\n",
        "                        new_pretrained['performer.net.layers.{}.{}.{}'.format(k.split('.')[3], 2 if lm_kwargs['cross_attend'] else 1, k.split('.', 6)[-1])] = pretrained[k]\n",
        "                    else:\n",
        "                        new_pretrained[k] = pretrained[k]\n",
        "            lm.load_state_dict(new_pretrained, strict=False)\n",
        "            print(\"Loaded pretrained language model: {}\".format(pretrained_lm))\n",
        "        self.lm = AutoregressiveWrapper(lm)\n",
        "        self.dec = AutoregressiveWrapper(dec)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate(self, instrumental, lyrics_start, lyrics_len, vocals_start, vocals_len, **kwargs):\n",
        "        enc_kwargs, lm_kwargs, dec_kwargs, kwargs = extract_and_set_enc_lm_dec_kwargs(kwargs)\n",
        "        instrumental_encodings = self.enc(instrumental, return_encodings = True, **enc_kwargs)\n",
        "        if self.lm_cross_attending:\n",
        "            lm_kwargs.setdefault('context', instrumental_encodings)\n",
        "            lm_kwargs.setdefault('context_mask', enc_kwargs['mask'])\n",
        "        lyrics_encodings, lyrics = self.lm.generate(lyrics_start, lyrics_len, return_also_encodings = True, **{**lm_kwargs, **kwargs})\n",
        "        vocals = self.dec.generate(vocals_start, vocals_len, context = instrumental_encodings, second_context = lyrics_encodings, **{**dec_kwargs, **kwargs})\n",
        "        return lyrics, vocals\n",
        "\n",
        "    def forward(self, instrumental, lyrics, vocals, **kwargs):\n",
        "        enc_kwargs, lm_kwargs, dec_kwargs, kwargs = extract_and_set_enc_lm_dec_kwargs(kwargs)\n",
        "        instrumental_encodings = self.enc(instrumental, return_encodings = True, **enc_kwargs)\n",
        "        if self.lm_cross_attending:\n",
        "            lm_kwargs.setdefault('context', instrumental_encodings)\n",
        "            lm_kwargs.setdefault('context_mask', enc_kwargs['mask'])\n",
        "        lyrics_encodings, lyrics_loss = self.lm(lyrics, return_also_encodings = True, **lm_kwargs)\n",
        "        vocals_loss = self.dec(vocals, context = instrumental_encodings, second_context = lyrics_encodings, **dec_kwargs)\n",
        "        return lyrics_loss + vocals_loss\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing decoupled_performer.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4PeFMLjhYtmO",
        "outputId": "14afd39d-fcae-4c1b-b1c7-dd83766303f7"
      },
      "source": [
        "%%writefile train_decoupled_performer.py\n",
        "\n",
        "import deepspeed\n",
        "from decoupled_performer import DecoupledPerformer\n",
        "import argparse\n",
        "import random\n",
        "import pandas as pd\n",
        "import json\n",
        "from itertools import cycle\n",
        "from pathlib import Path\n",
        "import os\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import DataLoader, Dataset, random_split\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "\n",
        "def get_arguments():\n",
        "    parser=argparse.ArgumentParser(description='Train Decoupled Performer on Lakh Midi Dataset Instruments-Lyrics-Vocal Melody')\n",
        "\n",
        "    parser.add_argument('--dataset-file', '-df', type=str, required=True,\n",
        "                        help='Dataset parquet file')\n",
        "\n",
        "    parser.add_argument('--vocabulary-prefix', '-v', type=str, default='',\n",
        "                        help='Prefix of the vocab files: <pref>_instrumental.vocab, <prf>_vocal.vocab')\n",
        "\n",
        "    parser.add_argument('--pretrained-language-model', '-plm', type=str,\n",
        "                        help='Pretrained language model to load')\n",
        "\n",
        "    parser.add_argument('--tokenizer', '-tok', type=str,\n",
        "                        help='Hugginface tokenizer to use')\n",
        "\n",
        "    parser.add_argument('--save-dir', '-sd', type=str, required=True,\n",
        "                        help='Directory to save checkpoints, states, event logs')\n",
        "    \n",
        "    parser.add_argument('--monophonic', '-m', default=False, action='store_true',\n",
        "                        help='Use monophonic instead of full instrumental input')\n",
        "\n",
        "    parser.add_argument('--max-instrumental-sequence-length', '-maxi', type=int, default=-1,\n",
        "                        help='If provided it will truncate samples with longer instrumental sequences')\n",
        "\n",
        "    parser.add_argument('--max-lyrics-sequence-length', '-maxl', type=int, default=1024,\n",
        "                        help='If provided it will truncate samples with longer lyrics sequences')\n",
        "    \n",
        "    parser.add_argument('--max-vocal-sequence-length', '-maxv', type=int, default=-1,\n",
        "                        help='If provided it will truncate samples with longer vocal melody sequences')\n",
        "    \n",
        "    parser.add_argument('--train-split', '-ts', type=float, default=0.9,\n",
        "                        help='Percentage of the dataset to use for training')\n",
        "\n",
        "    parser.add_argument('--epochs', '-e', type=int, default=20,\n",
        "                        help='Number of epochs')\n",
        "    \n",
        "    parser.add_argument('--validate-every', '-ve', type=int, default=200,\n",
        "                        help='Validate every n batches')\n",
        "    \n",
        "    parser.add_argument('--generate-every', '-ge', type=int, default=400,\n",
        "                        help='Generate every n batches')\n",
        "\n",
        "    parser.add_argument('--print-training-loss-every', '-ptle', type=int, default=20,\n",
        "                        help='It will average training loss and print it every n steps')\n",
        "\n",
        "    parser.add_argument('--validate-size', '-vs', type=int, default=40,\n",
        "                        help='Will calculate average of validation loss for n batches')\n",
        "\n",
        "    parser.add_argument('--validate-batch-size', '-vss', type=int, default=1,\n",
        "                        help='Batch size for validation dataset')\n",
        "\n",
        "    parser.add_argument('--checkpoints-per-epoch', '-cpp', type=int, default=3,\n",
        "                        help='How many checkpoints to keep per epoch')\n",
        "    \n",
        "    parser.add_argument('--local_rank', type=int, default=-1,\n",
        "                        help='Local rank passed from distributed launcher')\n",
        "    \n",
        "    parser = deepspeed.add_config_arguments(parser)\n",
        "\n",
        "    return parser.parse_args()\n",
        "\n",
        "\n",
        "class DecoupledDataset(Dataset):\n",
        "    def __init__(self, dataset_file, monophonic, vocabulary_prefix, max_instrumental_length, max_lyrics_length, max_vocal_length, tokenizer):\n",
        "        super().__init__()\n",
        "        instrumental_type = 'monophonic' if monophonic else 'instrumental'\n",
        "        with open('{}instrumental.vocab'.format(vocabulary_prefix), 'r') as f, \\\n",
        "            open('{}vocal.vocab'.format(vocabulary_prefix), 'r') as g: \n",
        "            self.instrumental_vocab = {w : l for l, w in enumerate(f.read().splitlines())}\n",
        "            self.reverse_instrumental_vocab = {l: w for w, l in self.instrumental_vocab.items()}\n",
        "            self.vocal_vocab = {w : l for l, w in enumerate(g.read().splitlines())}\n",
        "            self.reverse_vocal_vocab = {l: w for w, l in self.vocal_vocab.items()}\n",
        "        os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer, use_fast=True)\n",
        "            \n",
        "        df = pd.read_parquet(dataset_file)\n",
        "\n",
        "        self.files = list(df['file'])\n",
        "        self.instrumental = [self.encode(json.loads(f), seq_type='instrumental', max_length=max_instrumental_length) for f in df[instrumental_type]]\n",
        "        self.lyrics = []\n",
        "        self.vocals = []\n",
        "        for lyric, vocal in zip(df['lyrics'], df['vocal']):\n",
        "            l = json.loads(lyric)\n",
        "            v = json.loads(vocal)\n",
        "            encoded_lyrics, max_syllables = self.encode(l, seq_type='lyrics', max_length=max_lyrics_length)\n",
        "            self.lyrics.append(encoded_lyrics)\n",
        "            self.vocals.append(self.encode(v, seq_type='vocals', max_length=max_vocal_length, max_syllables=max_syllables))\n",
        "\n",
        "        self.max_instrumental_length = max([len(f) for f in self.instrumental])\n",
        "        self.max_lyrics_length = max([len(f) for f in self.lyrics])\n",
        "        self.max_vocal_length = max([len(f) for f in self.vocals])\n",
        "\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return (self.instrumental[index], self.lyrics[index], self.vocals[index]), self.files[index]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.files)\n",
        "\n",
        "    def truncate(self, sequence, max_length):\n",
        "        if max_length >= 0:\n",
        "            return sequence[:max_length]\n",
        "        return sequence\n",
        "\n",
        "    def encode(self, event_sequence, seq_type, max_length=-1, max_syllables=-1):\n",
        "        if seq_type == 'instrumental':\n",
        "            return torch.tensor([self.instrumental_vocab[e] for e in self.truncate(event_sequence, max_length - 1)] + [self.instrumental_vocab['<eos>']])\n",
        "        elif seq_type == 'lyrics':\n",
        "            tokenized = self.tokenizer(''.join(event_sequence), max_length=max_length - 2, truncation=True, return_overflowing_tokens=True)\n",
        "            if len(tokenized.encodings) == 2:\n",
        "                last_word_index = tokenized[0].word_ids[-1]\n",
        "                if last_word_index == tokenized[1].word_ids[0]:\n",
        "                    tokens = [tokenized[0].tokens[i] for i in range(len(tokenized[0])) if tokenized[0].word_ids[i] < last_word_index]\n",
        "                else:\n",
        "                    tokens = tokenized[0].tokens\n",
        "                size = len(self.tokenizer.convert_tokens_to_string(tokens).strip())\n",
        "                max_syllables = 0\n",
        "                chars = 0\n",
        "                for l in event_sequence:\n",
        "                    chars += len(l)\n",
        "                    if chars > size:\n",
        "                        break\n",
        "                    max_syllables += 1                \n",
        "                ids = self.tokenizer.convert_tokens_to_ids(tokens)\n",
        "            else:\n",
        "                ids = tokenized[0].ids\n",
        "            return torch.tensor([self.tokenizer.bos_token_id] + ids + [self.tokenizer.eos_token_id]), max_syllables\n",
        "        else:\n",
        "            if max_syllables >= 0:\n",
        "                last_index = -1\n",
        "                syllables = 0\n",
        "                for i, e in enumerate(event_sequence):\n",
        "                    if '_' not in e:\n",
        "                        syllables += 1\n",
        "                        if syllables > max_syllables:\n",
        "                            last_index = i\n",
        "                            break\n",
        "                if last_index >= 0:\n",
        "                    event_sequence = event_sequence[:last_index]\n",
        "            return torch.tensor([self.vocal_vocab['<bos>']] + [self.vocal_vocab[e] for e in self.truncate([e for e in event_sequence if '_' in e], max_length - 2)] + [self.vocal_vocab['<eos>']])\n",
        "\n",
        "    def decode(self, event_sequence, seq_type, mask=None):\n",
        "        size = len(event_sequence)\n",
        "        if mask is not None:\n",
        "            mask = mask.tolist()\n",
        "            true_size = len([v for v in mask if v])\n",
        "        else:\n",
        "            true_size = size\n",
        "        if seq_type == 'instrumental':\n",
        "            return [self.reverse_instrumental_vocab[i.item()] for i in event_sequence[:true_size]]\n",
        "        elif seq_type == 'lyrics':\n",
        "            return self.tokenizer.decode(event_sequence[:true_size])\n",
        "        else:\n",
        "            return [self.reverse_vocal_vocab[o.item()] for o in event_sequence[:true_size]]\n",
        "\n",
        "\n",
        "def collate_fn_zero_pad(batch):\n",
        "    data, files = zip(*batch)\n",
        "    instrumental, lyrics, vocals = zip(*data)\n",
        "    batch_size = len(files)\n",
        "\n",
        "    if batch_size == 1:\n",
        "        instrumental = instrumental[0].view(1, -1)\n",
        "        vocals = vocals[0].view(1, -1)\n",
        "        lyrics = lyrics[0].view(1, -1)\n",
        "        instrumental_masks = torch.ones_like(instrumental).bool()\n",
        "        vocal_masks = torch.ones_like(vocals).bool()\n",
        "        lyrics_masks = torch.ones_like(lyrics).bool()\n",
        "        return (instrumental.long(), instrumental_masks), (lyrics.long(), lyrics_masks), (vocals.long(), vocal_masks), files[0]\n",
        "\n",
        "    instrumental_lengths = [seq.size(0) for seq in instrumental]\n",
        "    instrumental_max_length = max(instrumental_lengths)\n",
        "    instrumental_masks = torch.arange(instrumental_max_length).view(1, -1).expand(batch_size, -1) < torch.tensor(instrumental_lengths).view(-1, 1)\n",
        "    padded_instrumental = torch.zeros(batch_size, instrumental_max_length)\n",
        "    for i, l in enumerate(instrumental_lengths):\n",
        "        padded_instrumental[i, :l] = instrumental[i]\n",
        "\n",
        "    lyrics_lengths = [seq.size(0) for seq in lyrics]\n",
        "    lyrics_max_length = max(lyrics_lengths)\n",
        "    lyrics_masks = torch.arange(lyrics_max_length).view(1, -1).expand(batch_size, -1) < torch.tensor(lyrics_lengths).view(-1, 1)\n",
        "    padded_lyrics = torch.zeros(batch_size, lyrics_max_length)\n",
        "    for i, l in enumerate(lyrics_lengths):\n",
        "        padded_lyrics[i, :l] = lyrics[i]\n",
        "\n",
        "    vocal_lengths = [seq.size(0) for seq in vocals]\n",
        "    vocal_max_length = max(vocal_lengths)\n",
        "    vocal_masks = torch.arange(vocal_max_length).view(1, -1).expand(batch_size, -1) < torch.tensor(vocal_lengths).view(-1, 1)\n",
        "    padded_vocals = torch.zeros(batch_size, vocal_max_length)\n",
        "    for i, l in enumerate(vocal_lengths):\n",
        "        padded_vocals[i, :l] = vocals[i]\n",
        "\n",
        "    return (padded_instrumental.long(), instrumental_masks), (padded_lyrics.long(), lyrics_masks), (padded_vocals.long(), vocal_masks), files\n",
        "\n",
        "\n",
        "def valid_structure_metric(sequence, vocab):\n",
        "    def get_valids_for_next(e, note_was_on):\n",
        "        if e == waits[-1]:\n",
        "            valid_events = waits + offs + boundaries + phonemes + ons\n",
        "        elif e in waits:\n",
        "            valid_events = offs + boundaries + phonemes + ons\n",
        "        elif e in ons:\n",
        "            note_was_on = True\n",
        "            valid_events = waits\n",
        "        elif e in offs:\n",
        "            note_was_on = False\n",
        "            valid_events = waits + boundaries + phonemes + ons\n",
        "        elif e in boundaries:\n",
        "            if e == boundaries[-1]:\n",
        "                valid_events = boundaries[:-1] + phonemes + ons\n",
        "            else:\n",
        "                valid_events = phonemes + ons\n",
        "        else:\n",
        "            valid_events = ons\n",
        "        return valid_events, note_was_on\n",
        "\n",
        "    sequence = sequence.tolist()\n",
        "    waits = [i for e, i in vocab.items() if e[:2] == 'W_']\n",
        "    ons = [i for e, i in vocab.items() if e[:3] == 'ON_']\n",
        "    offs = [vocab['_OFF_']]\n",
        "    boundaries = [vocab[e] for e in ['N_DL', 'N_L', 'N_W', '_C_']]\n",
        "    phonemes = [vocab['_R_']]\n",
        "    \n",
        "    valid_count = 0\n",
        "    valid_events = waits + boundaries\n",
        "    note_was_on = False\n",
        "    for e in sequence:\n",
        "        if e in valid_events and \\\n",
        "        (e not in ons or note_was_on == False) and \\\n",
        "        (e not in offs or note_was_on == True):\n",
        "            valid_count += 1\n",
        "        valid_events, note_was_on = get_valids_for_next(e, note_was_on)\n",
        "\n",
        "    size = len(sequence) - 1 if sequence[-1] == 2 else len(sequence)\n",
        "    if size == 0:\n",
        "        return 0\n",
        "    else:\n",
        "        return valid_count / size\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    args = get_arguments()\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    dataset = DecoupledDataset(dataset_file=args.dataset_file,\n",
        "                               monophonic=args.monophonic,\n",
        "                               vocabulary_prefix=args.vocabulary_prefix,\n",
        "                               max_instrumental_length=args.max_instrumental_sequence_length,\n",
        "                               max_lyrics_length=args.max_lyrics_sequence_length,\n",
        "                               max_vocal_length=args.max_vocal_sequence_length,\n",
        "                               tokenizer=args.tokenizer)\n",
        "\n",
        "    train_size = int(args.train_split * len(dataset))\n",
        "    val_size = len(dataset) - train_size\n",
        "    \n",
        "    torch.manual_seed(0)\n",
        "    train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "    train_log_dir = os.path.join(args.save_dir, 'train')\n",
        "    val_log_dir = os.path.join(args.save_dir, 'val')\n",
        "    Path(train_log_dir).mkdir(parents=True, exist_ok=True)\n",
        "    Path(val_log_dir).mkdir(parents=True, exist_ok=True)\n",
        "    writer_train = SummaryWriter(log_dir=train_log_dir)\n",
        "    writer_val = SummaryWriter(log_dir=val_log_dir)\n",
        "    \n",
        "    model = DecoupledPerformer(\n",
        "        dim = 768,\n",
        "        enc_heads = 6,\n",
        "        lm_heads = 12,\n",
        "        dec_heads = 6,\n",
        "        enc_depth = 6,\n",
        "        lm_depth = 6,\n",
        "        dec_depth = 6,\n",
        "        enc_ff_chunks = 10,\n",
        "        lm_ff_chunks = 1,\n",
        "        dec_ff_chunks = 10,\n",
        "        enc_num_tokens = len(dataset.instrumental_vocab),\n",
        "        lm_num_tokens = len(dataset.tokenizer),\n",
        "        dec_num_tokens = len(dataset.vocal_vocab),\n",
        "        enc_max_seq_len = dataset.max_instrumental_length,\n",
        "        lm_max_seq_len = args.max_lyrics_sequence_length,\n",
        "        dec_max_seq_len = dataset.max_vocal_length,\n",
        "        enc_emb_dropout = 0.1,\n",
        "        lm_emb_dropout = 0.1,\n",
        "        dec_emb_dropout = 0.1,\n",
        "        enc_ff_dropout = 0.1,\n",
        "        lm_ff_dropout = 0.1,\n",
        "        dec_ff_dropout = 0.1,\n",
        "        enc_attn_dropout = 0.1,\n",
        "        lm_attn_dropout = 0.1,\n",
        "        dec_attn_dropout = 0.1,\n",
        "        enc_tie_embed = True,\n",
        "        lm_tie_embed = True,\n",
        "        dec_tie_embed = True,\n",
        "        enc_reversible = True,\n",
        "        lm_reversible = True,\n",
        "        dec_reversible = True,\n",
        "        pretrained_lm = args.pretrained_language_model,\n",
        "        # lm_cross_attend = True\n",
        "    ).to(device)\n",
        "\n",
        "    model_engine, optimizer, trainloader, _ = deepspeed.initialize(args=args, model=model, model_parameters=model.parameters(),  training_data=train_dataset, collate_fn=collate_fn_zero_pad)\n",
        "    device = model_engine.local_rank\n",
        "\n",
        "    torch.manual_seed(torch.initial_seed())\n",
        "    val_loader_ = DataLoader(val_dataset, batch_size=args.validate_batch_size, shuffle=True, collate_fn=collate_fn_zero_pad)\n",
        "    val_loader = cycle(val_loader_)\n",
        "\n",
        "    num_batches = (len(train_dataset) + trainloader.batch_size - 1) // trainloader.batch_size\n",
        "\n",
        "    save_every = num_batches // args.checkpoints_per_epoch\n",
        "    save_at = 0\n",
        "    saving_steps = []\n",
        "    for _ in range(args.checkpoints_per_epoch - 1):\n",
        "        save_at += save_every\n",
        "        saving_steps.append(save_at)\n",
        "    saving_steps.append(num_batches - 1)\n",
        "\n",
        "    print(\"\\n\", \"Dataset maximum sequence lengths - Instrumental: {}, Lyrics: {}, Vocal: {}\".format(dataset.max_instrumental_length, dataset.max_lyrics_length, dataset.max_vocal_length), \"\\n\")\n",
        "    print(\"\\n\", \"Train Dataset - size: {}, batches: {}\".format(len(train_dataset), num_batches), \"\\n\")\n",
        "    print(\"\\n\", \"Validate Dataset - size: {}, batches: {}\".format(len(val_dataset), len(val_loader_)), \"\\n\")\n",
        "\n",
        "    checkpoint_name, client_state = model_engine.load_checkpoint(args.save_dir, load_module_strict=False)\n",
        "    # checkpoint_name = None\n",
        "\n",
        "    if checkpoint_name is not None:\n",
        "        print(\"\\nLoaded checkpoint: {}\\n\".format(checkpoint_name))        \n",
        "        i = client_state['i']\n",
        "        i += 1\n",
        "        epoch, step = divmod(i, num_batches)\n",
        "        print(\"Epoch: {}, step: {}, i: {}\".format(epoch, step, i))\n",
        "        if step == 0:\n",
        "            print(\"Starting next epoch...\")\n",
        "            rng = torch.get_rng_state()\n",
        "            trainloader = iter(trainloader)\n",
        "        else:\n",
        "            rng = torch.load(os.path.join(args.save_dir, 'rng_state.pt'))\n",
        "            torch.set_rng_state(rng)\n",
        "            trainloader = iter(trainloader)\n",
        "            print(\"Advancing dataloader...\")\n",
        "            for _ in range(step):\n",
        "                next(trainloader)\n",
        "    else:\n",
        "        print(\"\\nNo checkpoint found, training from scratch\\n\")\n",
        "        i = 0\n",
        "        step = 0\n",
        "        epoch = 0\n",
        "        rng = torch.get_rng_state()\n",
        "        trainloader = iter(trainloader)\n",
        "\n",
        "\n",
        "    for e in range(args.epochs - epoch):\n",
        "        running_loss = 0\n",
        "        running_loss_steps = 0\n",
        "        print(\"EPOCH: {}\".format(e + epoch))\n",
        "        while True:\n",
        "            try:\n",
        "                data = next(trainloader)\n",
        "            except StopIteration:\n",
        "                step = 0\n",
        "                rng = torch.get_rng_state()\n",
        "                trainloader = iter(trainloader)\n",
        "                break\n",
        "\n",
        "            model_engine.train()\n",
        "            (instrumental, instrumental_mask), (lyrics, lyrics_mask), (vocals, vocals_mask), _ = data\n",
        "            loss = model_engine(instrumental.to(device),\n",
        "                                lyrics.to(device),\n",
        "                                vocals.to(device),\n",
        "                                enc_mask=instrumental_mask.to(device),\n",
        "                                lm_mask=lyrics_mask.to(device),\n",
        "                                dec_mask=vocals_mask.to(device))\n",
        "            model_engine.backward(loss)\n",
        "            model_engine.step()\n",
        "            \n",
        "            running_loss += loss.item()\n",
        "            running_loss_steps += 1\n",
        "            if running_loss_steps == args.print_training_loss_every or step == 0:\n",
        "                avg_loss = running_loss / running_loss_steps\n",
        "                print(\"training loss: {}\".format(avg_loss))\n",
        "                writer_train.add_scalar(\"Loss\", avg_loss, i)\n",
        "                writer_train.flush()\n",
        "                running_loss = 0\n",
        "                running_loss_steps = 0\n",
        "\n",
        "            if step % args.validate_every == 0:\n",
        "                model_engine.eval()\n",
        "                with torch.no_grad():\n",
        "                    running_eval_loss = 0\n",
        "                    for _ in range(args.validate_size):\n",
        "                        (instrumental, instrumental_mask), (lyrics, lyrics_mask), (vocals, vocals_mask), _ = next(val_loader)\n",
        "                        loss = model_engine(instrumental.to(device),\n",
        "                                            lyrics.to(device),\n",
        "                                            vocals.to(device),\n",
        "                                            enc_mask=instrumental_mask.to(device),\n",
        "                                            lm_mask=lyrics_mask.to(device),\n",
        "                                            dec_mask=vocals_mask.to(device))\n",
        "                        running_eval_loss += loss.item()\n",
        "                    avg_eval_loss = running_eval_loss / args.validate_size\n",
        "                    print('\\n', f'validation loss: {avg_eval_loss}', '\\n')\n",
        "                    writer_val.add_scalar(\"Loss\", avg_eval_loss, i)\n",
        "                    writer_val.flush()\n",
        "                    running_eval_loss = 0\n",
        "\n",
        "            if step % args.generate_every == 0:\n",
        "                (instrumental, instrumental_mask), (expected_lyrics, expected_lyrics_mask), (expected_vocals, expected_vocals_mask), file = next(val_loader)\n",
        "                decoded_expected_lyrics = dataset.decode(expected_lyrics[0][1:], seq_type='lyrics', mask=expected_lyrics_mask[0][1:])\n",
        "                decoded_expected_vocals = dataset.decode(expected_vocals[0][1:], seq_type='vocals', mask=expected_vocals_mask[0][1:])\n",
        "\n",
        "                instrumental = instrumental[0].view(1, -1)\n",
        "                instrumental_mask = instrumental_mask[0].view(1, -1)\n",
        "                \n",
        "                # <bos> token\n",
        "                vocals_start = torch.ones(1,1).long()\n",
        "                lyrics_start = torch.full((1,1), dataset.tokenizer.bos_token_id).long()\n",
        "\n",
        "                lyrics, vocals = model_engine.module.generate(instrumental=instrumental.to(device),\n",
        "                                                              lyrics_start=lyrics_start.to(device),\n",
        "                                                              lyrics_len=args.max_lyrics_sequence_length//8,\n",
        "                                                              vocals_start=vocals_start.to(device),\n",
        "                                                              vocals_len=dataset.max_vocal_length//8,\n",
        "                                                              enc_mask=instrumental_mask.to(device),\n",
        "                                                              lm_eos_token=dataset.tokenizer.eos_token_id,\n",
        "                                                              dec_eos_token=2)\n",
        "                decoded_lyrics = dataset.decode(lyrics[0], seq_type='lyrics')\n",
        "                decoded_vocals = dataset.decode(vocals[0], seq_type='vocals')\n",
        "\n",
        "                with open(os.path.join(args.save_dir, 'outputs.txt'), 'a') as f:\n",
        "                    f.write(\"{}:\\n\\n{}\\n----------------\\n{}\\n----------------\\n{}\\n----------------\\n{}\\n----------------\\n\\n\"\\\n",
        "                                    .format(file, decoded_expected_lyrics, decoded_lyrics, decoded_expected_vocals, decoded_vocals))\n",
        "                \n",
        "                vsm = valid_structure_metric(vocals[0], dataset.vocal_vocab)\n",
        "                print(\"Valid Structure Metric: {}\".format(vsm))\n",
        "                expected_vsm = valid_structure_metric(expected_vocals[0][1:], dataset.vocal_vocab)\n",
        "                print(\"Expected Valid Structure Metric: {} (for control)\".format(expected_vsm))\n",
        "                writer_val.add_scalar(\"VSM\", vsm, i)\n",
        "                writer_val.flush()\n",
        "\n",
        "\n",
        "            if step in saving_steps:\n",
        "                loss_to_ckpt = avg_eval_loss if avg_eval_loss is not None else loss.item()\n",
        "                ckpt_id = \"{}-{}-{}\".format(e + epoch, i, loss_to_ckpt)\n",
        "                model_engine.save_checkpoint(args.save_dir, tag='latest_ckpt', client_state = {'i': i, 'step': step, 'epoch': e + epoch})\n",
        "                print(\"\\n{}\\n\".format(ckpt_id))\n",
        "                torch.save(rng, os.path.join(args.save_dir, 'rng_state.pt'))\n",
        "                torch.save(model_engine.module.state_dict(), os.path.join(args.save_dir, 'model.pt'))\n",
        "\n",
        "            i += 1\n",
        "            step += 1\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting train_decoupled_performer.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kf6fKZWNZAwk",
        "outputId": "bdc02d38-1abe-4c23-ed5f-5e3a59de2ac1"
      },
      "source": [
        "!deepspeed train_decoupled_performer.py -df drive/MyDrive/vanilla_performer/dataset_chords.parquet -v drive/MyDrive/decoupled_performer/decoupled_chords_ -plm drive/MyDrive/language_model/pretrained/lyrics_lm_1-epoch.pt -tok distilgpt2 -sd drive/MyDrive/decoupled_performer/chords -ve 400 -ge 600 -cpp 6 --deepspeed --deepspeed_config ds_config.json"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[2021-03-01 00:50:14,726] [WARNING] [runner.py:117:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.\n",
            "[2021-03-01 00:50:14,761] [INFO] [runner.py:355:main] cmd = /usr/bin/python3 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=29500 train_decoupled_performer.py -df drive/MyDrive/vanilla_performer/dataset_chords.parquet -v drive/MyDrive/decoupled_performer/decoupled_chords_ -plm drive/MyDrive/language_model/pretrained/lyrics_lm_1-epoch.pt -tok distilgpt2 -sd drive/MyDrive/decoupled_performer/chords -ve 400 -ge 600 -cpp 6 --deepspeed --deepspeed_config ds_config.json\n",
            "[2021-03-01 00:50:15,536] [INFO] [launch.py:71:main] 0 NCCL_VERSION 2.8.3\n",
            "[2021-03-01 00:50:15,536] [INFO] [launch.py:78:main] WORLD INFO DICT: {'localhost': [0]}\n",
            "[2021-03-01 00:50:15,536] [INFO] [launch.py:87:main] nnodes=1, num_local_procs=1, node_rank=0\n",
            "[2021-03-01 00:50:15,536] [INFO] [launch.py:99:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})\n",
            "[2021-03-01 00:50:15,536] [INFO] [launch.py:100:main] dist_world_size=1\n",
            "[2021-03-01 00:50:15,536] [INFO] [launch.py:103:main] Setting CUDA_VISIBLE_DEVICES=0\n",
            "2021-03-01 00:50:33.598630: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
            "Loaded pretrained language model: drive/MyDrive/language_model/pretrained/lyrics_lm_1-epoch.pt\n",
            "[2021-03-01 00:50:40,959] [INFO] [logging.py:60:log_dist] [Rank -1] DeepSpeed info: version=0.3.10, git-hash=unknown, git-branch=unknown\n",
            "[2021-03-01 00:50:40,959] [INFO] [distributed.py:40:init_distributed] Initializing torch distributed with backend: nccl\n",
            "[2021-03-01 00:50:41,036] [INFO] [engine.py:72:_initialize_parameter_parallel_groups] data_parallel_size: 1, parameter_parallel_size: 1\n",
            "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
            "Detected CUDA files, patching ldflags\n",
            "Emitting ninja build file /root/.cache/torch_extensions/fused_adam/build.ninja...\n",
            "Building extension module fused_adam...\n",
            "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
            "ninja: no work to do.\n",
            "Loading extension module fused_adam...\n",
            "Time to load fused_adam op: 0.38549137115478516 seconds\n",
            "[2021-03-01 00:50:42,586] [INFO] [engine.py:518:_configure_optimizer] Using DeepSpeed Optimizer param name adam as basic optimizer\n",
            "[2021-03-01 00:50:42,587] [INFO] [engine.py:521:_configure_optimizer] DeepSpeed Basic Optimizer = FusedAdam (\n",
            "Parameter Group 0\n",
            "    betas: [0.9, 0.98]\n",
            "    bias_correction: True\n",
            "    eps: 1e-08\n",
            "    lr: 0.001\n",
            "    weight_decay: 0.1\n",
            ")\n",
            "[2021-03-01 00:50:42,587] [INFO] [engine.py:551:_configure_optimizer] DeepSpeed Final Optimizer = FusedAdam (\n",
            "Parameter Group 0\n",
            "    betas: [0.9, 0.98]\n",
            "    bias_correction: True\n",
            "    eps: 1e-08\n",
            "    lr: 0.001\n",
            "    weight_decay: 0.1\n",
            ")\n",
            "[2021-03-01 00:50:42,587] [INFO] [engine.py:382:_configure_lr_scheduler] DeepSpeed using configured LR scheduler = WarmupLR\n",
            "[2021-03-01 00:50:42,587] [INFO] [logging.py:60:log_dist] [Rank 0] DeepSpeed LR Scheduler = <deepspeed.runtime.lr_schedules.WarmupLR object at 0x7f87f868c690>\n",
            "[2021-03-01 00:50:42,587] [INFO] [logging.py:60:log_dist] [Rank 0] step=0, skipped=0, lr=[0.001], mom=[[0.9, 0.98]]\n",
            "[2021-03-01 00:50:42,587] [INFO] [config.py:705:print] DeepSpeedEngine configuration:\n",
            "[2021-03-01 00:50:42,588] [INFO] [config.py:709:print]   activation_checkpointing_config  <deepspeed.runtime.activation_checkpointing.config.DeepSpeedActivationCheckpointingConfig object at 0x7f888cd7c810>\n",
            "[2021-03-01 00:50:42,588] [INFO] [config.py:709:print]   allreduce_always_fp32 ........ False\n",
            "[2021-03-01 00:50:42,588] [INFO] [config.py:709:print]   amp_enabled .................. False\n",
            "[2021-03-01 00:50:42,588] [INFO] [config.py:709:print]   amp_params ................... False\n",
            "[2021-03-01 00:50:42,588] [INFO] [config.py:709:print]   disable_allgather ............ False\n",
            "[2021-03-01 00:50:42,588] [INFO] [config.py:709:print]   dump_state ................... False\n",
            "[2021-03-01 00:50:42,588] [INFO] [config.py:709:print]   dynamic_loss_scale_args ...... None\n",
            "[2021-03-01 00:50:42,588] [INFO] [config.py:709:print]   elasticity_enabled ........... False\n",
            "[2021-03-01 00:50:42,588] [INFO] [config.py:709:print]   fp16_enabled ................. False\n",
            "[2021-03-01 00:50:42,588] [INFO] [config.py:709:print]   global_rank .................. 0\n",
            "[2021-03-01 00:50:42,589] [INFO] [config.py:709:print]   gradient_accumulation_steps .. 8\n",
            "[2021-03-01 00:50:42,589] [INFO] [config.py:709:print]   gradient_clipping ............ 0.5\n",
            "[2021-03-01 00:50:42,589] [INFO] [config.py:709:print]   gradient_predivide_factor .... 1.0\n",
            "[2021-03-01 00:50:42,589] [INFO] [config.py:709:print]   initial_dynamic_scale ........ 4294967296\n",
            "[2021-03-01 00:50:42,589] [INFO] [config.py:709:print]   loss_scale ................... 0\n",
            "[2021-03-01 00:50:42,589] [INFO] [config.py:709:print]   memory_breakdown ............. False\n",
            "[2021-03-01 00:50:42,589] [INFO] [config.py:709:print]   optimizer_legacy_fusion ...... False\n",
            "[2021-03-01 00:50:42,589] [INFO] [config.py:709:print]   optimizer_name ............... adam\n",
            "[2021-03-01 00:50:42,589] [INFO] [config.py:709:print]   optimizer_params ............. {'lr': 0.001, 'betas': [0.9, 0.98], 'eps': 1e-08, 'weight_decay': 0.1, 'adam_w_mode': True}\n",
            "[2021-03-01 00:50:42,589] [INFO] [config.py:709:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}\n",
            "[2021-03-01 00:50:42,589] [INFO] [config.py:709:print]   pld_enabled .................. False\n",
            "[2021-03-01 00:50:42,589] [INFO] [config.py:709:print]   pld_params ................... False\n",
            "[2021-03-01 00:50:42,589] [INFO] [config.py:709:print]   prescale_gradients ........... False\n",
            "[2021-03-01 00:50:42,589] [INFO] [config.py:709:print]   scheduler_name ............... WarmupLR\n",
            "[2021-03-01 00:50:42,589] [INFO] [config.py:709:print]   scheduler_params ............. {'warmup_min_lr': 0, 'warmup_max_lr': 0.001, 'warmup_num_steps': 100}\n",
            "[2021-03-01 00:50:42,589] [INFO] [config.py:709:print]   sparse_attention ............. None\n",
            "[2021-03-01 00:50:42,589] [INFO] [config.py:709:print]   sparse_gradients_enabled ..... False\n",
            "[2021-03-01 00:50:42,589] [INFO] [config.py:709:print]   steps_per_print .............. 20\n",
            "[2021-03-01 00:50:42,589] [INFO] [config.py:709:print]   tensorboard_enabled .......... False\n",
            "[2021-03-01 00:50:42,590] [INFO] [config.py:709:print]   tensorboard_job_name ......... DeepSpeedJobName\n",
            "[2021-03-01 00:50:42,590] [INFO] [config.py:709:print]   tensorboard_output_path ...... \n",
            "[2021-03-01 00:50:42,590] [INFO] [config.py:709:print]   train_batch_size ............. 8\n",
            "[2021-03-01 00:50:42,590] [INFO] [config.py:709:print]   train_micro_batch_size_per_gpu  1\n",
            "[2021-03-01 00:50:42,590] [INFO] [config.py:709:print]   wall_clock_breakdown ......... False\n",
            "[2021-03-01 00:50:42,590] [INFO] [config.py:709:print]   world_size ................... 1\n",
            "[2021-03-01 00:50:42,590] [INFO] [config.py:709:print]   zero_allow_untested_optimizer  False\n",
            "[2021-03-01 00:50:42,590] [INFO] [config.py:709:print]   zero_config .................. {\n",
            "    \"allgather_bucket_size\": 500000000,\n",
            "    \"allgather_partitions\": true,\n",
            "    \"contiguous_gradients\": false,\n",
            "    \"cpu_offload\": false,\n",
            "    \"elastic_checkpoint\": true,\n",
            "    \"load_from_fp32_weights\": true,\n",
            "    \"overlap_comm\": false,\n",
            "    \"reduce_bucket_size\": 500000000,\n",
            "    \"reduce_scatter\": true,\n",
            "    \"stage\": 0\n",
            "}\n",
            "[2021-03-01 00:50:42,590] [INFO] [config.py:709:print]   zero_enabled ................. False\n",
            "[2021-03-01 00:50:42,590] [INFO] [config.py:709:print]   zero_optimization_stage ...... 0\n",
            "[2021-03-01 00:50:42,591] [INFO] [config.py:715:print]   json = {\n",
            "    \"gradient_accumulation_steps\":8,\n",
            "    \"gradient_clipping\":0.5,\n",
            "    \"optimizer\":{\n",
            "        \"params\":{\n",
            "            \"adam_w_mode\":true,\n",
            "            \"betas\":[\n",
            "                0.9,\n",
            "                0.98\n",
            "            ],\n",
            "            \"eps\":1e-08,\n",
            "            \"lr\":0.001,\n",
            "            \"weight_decay\":0.1\n",
            "        },\n",
            "        \"type\":\"Adam\"\n",
            "    },\n",
            "    \"scheduler\":{\n",
            "        \"params\":{\n",
            "            \"warmup_max_lr\":0.001,\n",
            "            \"warmup_min_lr\":0,\n",
            "            \"warmup_num_steps\":100\n",
            "        },\n",
            "        \"type\":\"WarmupLR\"\n",
            "    },\n",
            "    \"steps_per_print\":20,\n",
            "    \"train_batch_size\":8\n",
            "}\n",
            "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
            "Emitting ninja build file /root/.cache/torch_extensions/utils/build.ninja...\n",
            "Building extension module utils...\n",
            "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
            "ninja: no work to do.\n",
            "Loading extension module utils...\n",
            "Time to load utils op: 0.392305850982666 seconds\n",
            "\n",
            " Dataset maximum sequence lengths - Instrumental: 11731, Lyrics: 1024, Vocal: 4816 \n",
            "\n",
            "\n",
            " Train Dataset - size: 7654, batches: 7654 \n",
            "\n",
            "\n",
            " Validate Dataset - size: 851, batches: 851 \n",
            "\n",
            "[2021-03-01 00:50:42,984] [WARNING] [engine.py:1252:load_checkpoint] Unable to find latest file at drive/MyDrive/decoupled_performer/chords/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.\n",
            "\n",
            "No checkpoint found, training from scratch\n",
            "\n",
            "EPOCH: 0\n",
            "training loss: 492.6674499511719\n",
            "\n",
            " validation loss: 504.641015625 \n",
            "\n",
            "Valid Structure Metric: 0.0\n",
            "Expected Valid Structure Metric: 1.0 (for control)\n",
            "[2021-03-01 00:53:07,132] [INFO] [timer.py:166:stop] 0/20, SamplesPerSec=0.23584825446298413\n",
            "training loss: 263.0371429443359\n",
            "[2021-03-01 00:54:27,372] [INFO] [timer.py:166:stop] 0/40, SamplesPerSec=0.24272202545867957\n",
            "training loss: 136.48504486083985\n",
            "[2021-03-01 00:55:42,084] [INFO] [timer.py:166:stop] 0/60, SamplesPerSec=0.25079366774670203\n",
            "training loss: 70.30743026733398\n",
            "[2021-03-01 00:57:04,346] [INFO] [timer.py:166:stop] 0/80, SamplesPerSec=0.24878362274485033\n",
            "training loss: 61.696801376342776\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}