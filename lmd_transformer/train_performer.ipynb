{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "train_performer.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "41EWXAeL2nYs",
        "outputId": "445732e6-16f5-45f8-9763-311b142b89d1"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!pip install drive/MyDrive/lmd_transformer/pytorch_fast_transformers-0.3.0-cp36-cp36m-linux_x86_64.whl\n",
        "!pip install performer-pytorch --upgrade\n",
        "!pip install deepspeed\n",
        "!pip install pytorch-extension\n",
        "!pip install allennlp"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Requirement already satisfied: pytorch-fast-transformers==0.3.0 from file:///content/drive/MyDrive/lmd_transformer/pytorch_fast_transformers-0.3.0-cp36-cp36m-linux_x86_64.whl in /usr/local/lib/python3.6/dist-packages (0.3.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from pytorch-fast-transformers==0.3.0) (1.7.0+cu101)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch->pytorch-fast-transformers==0.3.0) (0.8)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch->pytorch-fast-transformers==0.3.0) (3.7.4.3)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch->pytorch-fast-transformers==0.3.0) (0.16.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch->pytorch-fast-transformers==0.3.0) (1.18.5)\n",
            "Requirement already up-to-date: performer-pytorch in /usr/local/lib/python3.6/dist-packages (0.12.7)\n",
            "Requirement already satisfied, skipping upgrade: torch>=1.6 in /usr/local/lib/python3.6/dist-packages (from performer-pytorch) (1.7.0+cu101)\n",
            "Requirement already satisfied, skipping upgrade: pytorch-fast-transformers>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from performer-pytorch) (0.3.0)\n",
            "Requirement already satisfied, skipping upgrade: local-attention>=1.1.1 in /usr/local/lib/python3.6/dist-packages (from performer-pytorch) (1.1.1)\n",
            "Requirement already satisfied, skipping upgrade: einops>=0.3 in /usr/local/lib/python3.6/dist-packages (from performer-pytorch) (0.3.0)\n",
            "Requirement already satisfied, skipping upgrade: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.6->performer-pytorch) (0.16.0)\n",
            "Requirement already satisfied, skipping upgrade: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch>=1.6->performer-pytorch) (0.8)\n",
            "Requirement already satisfied, skipping upgrade: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch>=1.6->performer-pytorch) (3.7.4.3)\n",
            "Requirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.6/dist-packages (from torch>=1.6->performer-pytorch) (1.18.5)\n",
            "Requirement already satisfied: deepspeed in /usr/local/lib/python3.6/dist-packages (0.3.8)\n",
            "Requirement already satisfied: ninja in /usr/local/lib/python3.6/dist-packages (from deepspeed) (1.10.0.post2)\n",
            "Requirement already satisfied: torch>=1.2 in /usr/local/lib/python3.6/dist-packages (from deepspeed) (1.7.0+cu101)\n",
            "Requirement already satisfied: tensorboardX==1.8 in /usr/local/lib/python3.6/dist-packages (from deepspeed) (1.8)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from deepspeed) (4.41.1)\n",
            "Requirement already satisfied: torchvision>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from deepspeed) (0.8.1+cu101)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.2->deepspeed) (0.16.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch>=1.2->deepspeed) (3.7.4.3)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch>=1.2->deepspeed) (0.8)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch>=1.2->deepspeed) (1.18.5)\n",
            "Requirement already satisfied: protobuf>=3.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorboardX==1.8->deepspeed) (3.12.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from tensorboardX==1.8->deepspeed) (1.15.0)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision>=0.4.0->deepspeed) (7.0.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.2.0->tensorboardX==1.8->deepspeed) (50.3.2)\n",
            "Requirement already satisfied: pytorch-extension in /usr/local/lib/python3.6/dist-packages (0.1)\n",
            "Collecting allennlp\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/cb/42/ee4abac910d0becba85ef0c2cfa2d5a954d986a1356708db5a0cd911783c/allennlp-1.2.2-py3-none-any.whl (505kB)\n",
            "\u001b[K     |████████████████████████████████| 512kB 13.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests>=2.18 in /usr/local/lib/python3.6/dist-packages (from allennlp) (2.23.0)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from allennlp) (0.8)\n",
            "Requirement already satisfied: tensorboardX>=1.2 in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.8)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.18.5)\n",
            "Requirement already satisfied: spacy<2.4,>=2.1.0 in /usr/local/lib/python3.6/dist-packages (from allennlp) (2.2.4)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from allennlp) (2.10.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.4.1)\n",
            "Collecting jsonnet>=0.10.0; sys_platform != \"win32\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/42/40/6f16e5ac994b16fa71c24310f97174ce07d3a97b433275589265c6b94d2b/jsonnet-0.17.0.tar.gz (259kB)\n",
            "\u001b[K     |████████████████████████████████| 266kB 56.2MB/s \n",
            "\u001b[?25hCollecting overrides==3.1.0\n",
            "  Downloading https://files.pythonhosted.org/packages/ff/b1/10f69c00947518e6676bbd43e739733048de64b8dd998e9c2d5a71f44c5d/overrides-3.1.0.tar.gz\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.6/dist-packages (from allennlp) (3.6.4)\n",
            "Collecting transformers<3.6,>=3.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3a/83/e74092e7f24a08d751aa59b37a9fc572b2e4af3918cb66f7766c3affb1b4/transformers-3.5.1-py3-none-any.whl (1.3MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3MB 18.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from allennlp) (0.22.2.post1)\n",
            "Requirement already satisfied: filelock<3.1,>=3.0 in /usr/local/lib/python3.6/dist-packages (from allennlp) (3.0.12)\n",
            "Requirement already satisfied: torch<1.8.0,>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.7.0+cu101)\n",
            "Collecting boto3<2.0,>=1.14\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/76/69/1ae307bbfbc3df15778ad1568f9b2def472fbcc07ec81479d148370abf89/boto3-1.16.33.tar.gz (97kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 13.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (from allennlp) (3.2.5)\n",
            "Collecting jsonpickle\n",
            "  Downloading https://files.pythonhosted.org/packages/ee/d5/1cc282dc23346a43aab461bf2e8c36593aacd34242bee1a13fa750db0cfe/jsonpickle-1.4.2-py2.py3-none-any.whl\n",
            "Requirement already satisfied: tqdm>=4.19 in /usr/local/lib/python3.6/dist-packages (from allennlp) (4.41.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18->allennlp) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18->allennlp) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18->allennlp) (2020.11.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18->allennlp) (2.10)\n",
            "Requirement already satisfied: protobuf>=3.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorboardX>=1.2->allennlp) (3.12.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from tensorboardX>=1.2->allennlp) (1.15.0)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4,>=2.1.0->allennlp) (7.4.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4,>=2.1.0->allennlp) (1.1.3)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4,>=2.1.0->allennlp) (0.8.0)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4,>=2.1.0->allennlp) (1.0.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy<2.4,>=2.1.0->allennlp) (50.3.2)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4,>=2.1.0->allennlp) (0.4.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4,>=2.1.0->allennlp) (2.0.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4,>=2.1.0->allennlp) (1.0.4)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4,>=2.1.0->allennlp) (3.0.4)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4,>=2.1.0->allennlp) (1.0.4)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (20.3.0)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (1.4.0)\n",
            "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (0.7.1)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (1.9.0)\n",
            "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (8.6.0)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 48.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers<3.6,>=3.4->allennlp) (20.4)\n",
            "Collecting sentencepiece==0.1.91\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 48.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers<3.6,>=3.4->allennlp) (2019.12.20)\n",
            "Collecting tokenizers==0.9.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4c/34/b39eb9994bc3c999270b69c9eea40ecc6f0e97991dba28282b9fd32d44ee/tokenizers-0.9.3-cp36-cp36m-manylinux1_x86_64.whl (2.9MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9MB 50.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->allennlp) (0.17.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch<1.8.0,>=1.6.0->allennlp) (0.16.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch<1.8.0,>=1.6.0->allennlp) (3.7.4.3)\n",
            "Collecting botocore<1.20.0,>=1.19.33\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/76/3e/5f64c7b492312069520d219ed56ce6b3d1821dab8251c18d25728578b58e/botocore-1.19.33-py2.py3-none-any.whl (7.0MB)\n",
            "\u001b[K     |████████████████████████████████| 7.0MB 48.1MB/s \n",
            "\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1\n",
            "  Downloading https://files.pythonhosted.org/packages/07/cb/5f001272b6faeb23c1c9e0acc04d48eaaf5c862c17709d20e3469c6e0139/jmespath-0.10.0-py2.py3-none-any.whl\n",
            "Collecting s3transfer<0.4.0,>=0.3.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/69/79/e6afb3d8b0b4e96cefbdc690f741d7dd24547ff1f94240c997a26fa908d3/s3transfer-0.3.3-py2.py3-none-any.whl (69kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 11.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from jsonpickle->allennlp) (2.0.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers<3.6,>=3.4->allennlp) (7.1.2)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers<3.6,>=3.4->allennlp) (2.4.7)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.20.0,>=1.19.33->boto3<2.0,>=1.14->allennlp) (2.8.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->jsonpickle->allennlp) (3.4.0)\n",
            "Building wheels for collected packages: jsonnet, overrides, boto3, sacremoses\n",
            "  Building wheel for jsonnet (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jsonnet: filename=jsonnet-0.17.0-cp36-cp36m-linux_x86_64.whl size=3387920 sha256=f5c41b4e63e7bac2377d9a6f6498ab06e9dbb1a5c2f0804f6e6a047fcf102eeb\n",
            "  Stored in directory: /root/.cache/pip/wheels/26/7a/37/7dbcc30a6b4efd17b91ad1f0128b7bbf84813bd4e1cfb8c1e3\n",
            "  Building wheel for overrides (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for overrides: filename=overrides-3.1.0-cp36-none-any.whl size=10174 sha256=f927092e0a789e997de7a78a62fbce4a271620148314722e4e44ddae93ae92db\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/24/13/6ef8600e6f147c95e595f1289a86a3cc82ed65df57582c65a9\n",
            "  Building wheel for boto3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for boto3: filename=boto3-1.16.33-py2.py3-none-any.whl size=128451 sha256=8732127ec36d1494bd6ca9bf44459050f27140779d42f8d968b74c17f316c7b5\n",
            "  Stored in directory: /root/.cache/pip/wheels/28/c0/ba/5f9cce3a8686eea945fd594ae8fcdec24b1bd7402d3d065a47\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893257 sha256=4c520b2e93e85e2c358f53e5287a79a4dd86a48a5c4a97afae616ce3de8b5397\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built jsonnet overrides boto3 sacremoses\n",
            "\u001b[31mERROR: botocore 1.19.33 has requirement urllib3<1.27,>=1.25.4; python_version != \"3.4\", but you'll have urllib3 1.24.3 which is incompatible.\u001b[0m\n",
            "Installing collected packages: jsonnet, overrides, sacremoses, sentencepiece, tokenizers, transformers, jmespath, botocore, s3transfer, boto3, jsonpickle, allennlp\n",
            "Successfully installed allennlp-1.2.2 boto3-1.16.33 botocore-1.19.33 jmespath-0.10.0 jsonnet-0.17.0 jsonpickle-1.4.2 overrides-3.1.0 s3transfer-0.3.3 sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.9.3 transformers-3.5.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5GPe_Vj5fUYd",
        "outputId": "6103df0c-db54-4087-a1ad-7f2ac00df4f5"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wed Dec  9 23:01:11 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 455.45.01    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   46C    P8    10W /  70W |      0MiB / 15079MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XUKjfF4fgQPs",
        "outputId": "21ddd52e-2257-4679-ef8b-ed667a720c5a"
      },
      "source": [
        "!ls drive/MyDrive/lmd_transformer/"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0-1999-9000-4.71875\n",
            "0-2999-10000-4.23046875\n",
            "0-3999-11000-4.765625\n",
            "0.999.5.18359375\n",
            "0-999-8000-4.68359375\n",
            "6000.6000.4.40234375\n",
            "dataset.parquet\n",
            "instrumental.vocab\n",
            "latest\n",
            "performer_ending-epoch0\n",
            "performer_epoch0\n",
            "pytorch_fast_transformers-0.3.0-cp36-cp36m-linux_x86_64.whl\n",
            "small_dataset.parquet\n",
            "small_instrumental.vocab\n",
            "small_vocal.vocab\n",
            "tensorboard\n",
            "vocal.vocab\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ChU7fY2jfdH5",
        "outputId": "d639a116-7664-4f8d-a390-087eea4c48f0"
      },
      "source": [
        "%%writefile ds_config.json\n",
        "\n",
        "{\n",
        "  \"train_batch_size\": 32,\n",
        "  \"gradient_accumulation_steps\": 16,\n",
        "  \"steps_per_print\": 20,\n",
        "  \"max_grad_norm\": 0.5,\n",
        "  \"optimizer\": {\n",
        "    \"type\": \"Adam\",\n",
        "    \"params\": {\n",
        "      \"lr\": 0.0002,\n",
        "      \"betas\": [\n",
        "        0.8,\n",
        "        0.999\n",
        "      ],\n",
        "      \"eps\": 1e-8\n",
        "    }\n",
        "  },\n",
        "  \"amp\": {\n",
        "    \"enabled\": true,\n",
        "    \"opt_level\": \"O2\"\n",
        "  },\n",
        "  \"scheduler\": {\n",
        "    \"type\": \"WarmupLR\",\n",
        "    \"params\": {\n",
        "      \"warmup_min_lr\": 0,\n",
        "      \"warmup_max_lr\": 0.0002,\n",
        "      \"warmup_num_steps\": 1000\n",
        "    }\n",
        "  }\n",
        "}"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting ds_config.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vvcn21V2FFq5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "51a21661-09ad-478f-a495-240b7afb82e7"
      },
      "source": [
        "%%writefile train_performer.py\n",
        "\n",
        "import deepspeed\n",
        "from performer_pytorch import PerformerEncDec\n",
        "import argparse\n",
        "import random\n",
        "import pandas as pd\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "from allennlp.training.metrics import BLEU\n",
        "from itertools import cycle\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import DataLoader, Dataset, random_split\n",
        "\n",
        "\n",
        "def get_arguments():\n",
        "    parser=argparse.ArgumentParser(description='Lakh Midi Dataset Instruments-Vocals')\n",
        "\n",
        "    parser.add_argument('--dataset-file', '-df', type=str, required=True,\n",
        "                        help='Dataset parquet file')\n",
        "\n",
        "    parser.add_argument('--vocabulary-prefix', '-v', type=str, default='',\n",
        "                        help='Prefix of the vocab files: <pref>_instrumental.vocab, <prf>_vocal.vocab')\n",
        "    \n",
        "    parser.add_argument('--monophonic', '-m', default=False, action='store_true',\n",
        "                        help='Use monophonic instead of full instrumental input')\n",
        "\n",
        "    parser.add_argument('--max-input-sequence-length', '-maxi', type=int, default=-1,\n",
        "                        help='If provided it will skip samples with longer input sequences')\n",
        "    \n",
        "    parser.add_argument('--max-output-sequence-length', '-maxo', type=int, default=-1,\n",
        "                        help='If provided it will skip samples with longer output sequences')\n",
        "    \n",
        "    parser.add_argument('--train-split', '-ts', type=float, default=0.9,\n",
        "                        help='Percentage of the dataset to use for training')\n",
        "\n",
        "    parser.add_argument('--epochs', '-e', type=int, default=20,\n",
        "                        help='Number of epochs')\n",
        "    \n",
        "    parser.add_argument('--validate-every', '-ve', type=int, default=200,\n",
        "                        help='Validate every n batches')\n",
        "    \n",
        "    parser.add_argument('--generate-every', '-ge', type=int, default=500,\n",
        "                        help='Generate every n batches')\n",
        "    \n",
        "    parser.add_argument('--local_rank', type=int, default=-1,\n",
        "                        help='Local rank passed from distributed launcher')\n",
        "    \n",
        "    parser = deepspeed.add_config_arguments(parser)\n",
        "\n",
        "    return parser.parse_args()\n",
        "\n",
        "\n",
        "class MidiDataset(Dataset):\n",
        "    def __init__(self, dataset_file, monophonic, vocabulary_prefix, max_input_length, max_output_length):\n",
        "        super().__init__()\n",
        "        input_type = 'monophonic' if monophonic else 'instrumental'\n",
        "        with open('{}instrumental.vocab'.format(vocabulary_prefix), 'r') as f, \\\n",
        "            open('{}vocal.vocab'.format(vocabulary_prefix), 'r') as g: \n",
        "            self.input_vocab = {w : l for l, w in enumerate(f.read().splitlines())}\n",
        "            self.reverse_input_vocab = {l: w for w, l in self.input_vocab.items()}\n",
        "            self.output_vocab = {w : l for l, w in enumerate(g.read().splitlines())}\n",
        "            self.reverse_output_vocab = {l: w for w, l in self.output_vocab.items()}\n",
        "            \n",
        "        df = pd.read_parquet(dataset_file, columns=['vocal', input_type])\n",
        "        \n",
        "        inp = [self.encode(json.loads(f) + ['<eos>'], is_input=True) for f in df[input_type]]\n",
        "        out = [self.encode(['<bos>'] + json.loads(f) + ['<eos>'], is_input=False) for f in df['vocal']]\n",
        "\n",
        "        if max_input_length < 0 and max_output_length < 0:\n",
        "            self.input = inp\n",
        "            self.output = out\n",
        "        else:\n",
        "            self.input = []\n",
        "            self.output = []\n",
        "            for idx in range(len(inp)):\n",
        "                input_sample = inp[idx]\n",
        "                output_sample = out[idx]\n",
        "                if (max_input_length >= 0 and len(input_sample) > max_input_length) or \\\n",
        "                   (max_output_length >= 0 and len(output_sample) > max_output_length):\n",
        "                   continue\n",
        "                else:\n",
        "                    self.input.append(input_sample)\n",
        "                    self.output.append(output_sample)\n",
        "\n",
        "        self.max_input_length = max([len(f) for f in self.input])\n",
        "        self.max_output_length = max([len(f) for f in self.output])\n",
        "\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return (self.input[index], self.output[index])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input)\n",
        "\n",
        "    def encode(self, event_sequence, is_input):\n",
        "        if is_input:\n",
        "            return torch.tensor([self.input_vocab[i] for i in event_sequence])\n",
        "        else:\n",
        "            return torch.tensor([self.output_vocab[i] for i in event_sequence])\n",
        "\n",
        "    def decode(self, event_sequence, is_input, mask=None):\n",
        "        size = len(event_sequence)\n",
        "        if mask is not None:\n",
        "          mask = mask.tolist()\n",
        "          true_size = len([v for v in mask if v])\n",
        "        else:\n",
        "          true_size = size\n",
        "        if is_input:\n",
        "            return \",\".join([self.reverse_input_vocab[i.item()] for i in event_sequence[:true_size]])\n",
        "        else:\n",
        "            return \",\".join([self.reverse_output_vocab[o.item()] for o in event_sequence[:true_size]])\n",
        "\n",
        "def collate_fn_zero_pad(batch):\n",
        "    inputs, outputs = zip(*batch)\n",
        "    batch_size = len(inputs)\n",
        "\n",
        "    if batch_size == 1:\n",
        "      inputs = inputs[0].view(1, -1)\n",
        "      outputs = outputs[0].view(1, -1)\n",
        "      input_masks = torch.ones_like(inputs).bool()\n",
        "      output_masks = torch.ones_like(outputs).bool()\n",
        "      return (inputs.long(), input_masks), (outputs.long(), output_masks)\n",
        "\n",
        "    input_lengths = [seq.size(0) for seq in inputs]\n",
        "    input_max_length = max(input_lengths)\n",
        "    input_masks = torch.arange(input_max_length).view(1, -1).expand(batch_size, -1) < torch.tensor(input_lengths).view(-1, 1)\n",
        "    padded_inputs = torch.zeros(batch_size, input_max_length)\n",
        "    for i, l in enumerate(input_lengths):\n",
        "        padded_inputs[i, :l] = inputs[i]\n",
        "\n",
        "    output_lengths = [seq.size(0) for seq in outputs]\n",
        "    output_max_length = max(output_lengths)\n",
        "    output_masks = torch.arange(output_max_length).view(1, -1).expand(batch_size, -1) < torch.tensor(output_lengths).view(-1, 1)\n",
        "    padded_outputs = torch.zeros(batch_size, output_max_length)\n",
        "    for i, l in enumerate(output_lengths):\n",
        "        padded_outputs[i, :l] = outputs[i]\n",
        "\n",
        "    return (padded_inputs.long(), input_masks), (padded_outputs.long(), output_masks)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    args = get_arguments()\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    dataset = MidiDataset(dataset_file=args.dataset_file,\n",
        "                          monophonic=args.monophonic,\n",
        "                          vocabulary_prefix=args.vocabulary_prefix,\n",
        "                          max_input_length=args.max_input_sequence_length,\n",
        "                          max_output_length=args.max_output_sequence_length)\n",
        "\n",
        "    train_size = int(args.train_split * len(dataset))\n",
        "    val_size = len(dataset) - train_size\n",
        "    \n",
        "    torch.manual_seed(0)\n",
        "    train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "    writer = SummaryWriter(log_dir='drive/MyDrive/lmd_transformer/small_train/')\n",
        "    bleu = BLEU()\n",
        "\n",
        "    model = PerformerEncDec(\n",
        "        dim = 1024,\n",
        "        enc_heads = 8,\n",
        "        dec_heads = 8,\n",
        "        enc_depth = 6,\n",
        "        dec_depth = 6,\n",
        "        enc_num_tokens = len(dataset.input_vocab),\n",
        "        dec_num_tokens = len(dataset.output_vocab),\n",
        "        enc_max_seq_len = dataset.max_input_length,\n",
        "        dec_max_seq_len = dataset.max_output_length,\n",
        "        enc_emb_dropout = 0.1,\n",
        "        dec_emb_dropout = 0.1,\n",
        "        enc_ff_dropout = 0.1,\n",
        "        dec_ff_dropout = 0.1,\n",
        "        enc_attn_dropout = 0.1,\n",
        "        dec_attn_dropout = 0.1,\n",
        "        enc_reversible = True,\n",
        "        dec_reversible = True\n",
        "    ).to(device)\n",
        "\n",
        "    model_engine, optimizer, trainloader, _ = deepspeed.initialize(args=args, model=model, model_parameters=model.parameters(),  training_data=train_dataset, collate_fn=collate_fn_zero_pad)\n",
        "    device = model_engine.local_rank\n",
        "    \n",
        "    torch.manual_seed(torch.initial_seed())\n",
        "    val_loader_ = DataLoader(val_dataset, batch_size=1, shuffle=True, collate_fn=collate_fn_zero_pad)\n",
        "    val_loader = cycle(val_loader_)\n",
        "    \n",
        "    print(\"Dataset maximum sequence lengths - Input: {}, Output: {}\".format(dataset.max_input_length, dataset.max_output_length))\n",
        "    print(\"Train Dataset - size: {}, batches: {}\".format(len(train_dataset), len(trainloader)))\n",
        "    print(\"Validate Dataset - size: {}, batches: {}\".format(len(val_dataset), len(val_loader_)))\n",
        "\n",
        "    i = None\n",
        "    # checkpoint_name, client_state = model_engine.load_checkpoint('drive/MyDrive/lmd_transformer/small_train/', tag='latest')\n",
        "    # print(\"Loaded checkpoint: {}\".format(checkpoint_name))\n",
        "    \n",
        "    # i = client_state['i']\n",
        "    # i += 1\n",
        "    # step, epoch = divmod(i, len(trainloader))\n",
        "    # print(\"Epoch: {}, step: {}, i: {}\".format(epoch, step, i))\n",
        "    # print(\"Advancing dataloader...\")\n",
        "    # for _ in tqdm(range(step)):\n",
        "    #   next(iter(trainloader))\n",
        "\n",
        "    if i is None:\n",
        "      i = 0\n",
        "      step = 0\n",
        "      epoch = 0\n",
        "\n",
        "    for e in range(args.epochs - epoch):\n",
        "        running_loss = 0\n",
        "        print(\"EPOCH: {}\".format(e + epoch))\n",
        "        for step, data in enumerate(trainloader):\n",
        "            model_engine.train()\n",
        "            (inp, inp_mask), (out, out_mask) = data\n",
        "            loss = model_engine(inp.to(device), out.to(device), enc_mask=inp_mask.to(device), dec_mask=out_mask.to(device), return_loss=True)\n",
        "            model_engine.backward(loss)\n",
        "            model_engine.step()\n",
        "            running_loss += loss.item()\n",
        "            if step % 20 == 0:\n",
        "              avg_loss = running_loss / 20 if step > 0 else running_loss\n",
        "              print(\"training loss: {}\".format(avg_loss))\n",
        "              writer.add_scalar(\"Loss/train\", avg_loss, i)\n",
        "              writer.flush()\n",
        "              running_loss = 0\n",
        "\n",
        "            if step % args.validate_every == 0:\n",
        "                model_engine.eval()\n",
        "                with torch.no_grad():\n",
        "                    running_eval_loss = 0\n",
        "                    for _ in range(40):\n",
        "                        (inp, inp_mask), (out, out_mask) = next(val_loader)\n",
        "                        loss = model_engine(inp.to(device), out.to(device), return_loss=True, enc_mask=inp_mask.to(device), dec_mask=out_mask.to(device))\n",
        "                        running_eval_loss += loss.item()\n",
        "                    print('\\n', f'validation loss: {running_eval_loss / 40}', '\\n')\n",
        "                    writer.add_scalar(\"Loss/evaluate\", running_eval_loss / 40, i)\n",
        "                    writer.flush()\n",
        "\n",
        "            if step % args.generate_every == 0:\n",
        "                (inp, inp_mask), (expected_out, expected_out_mask) = next(val_loader)\n",
        "                print(dataset.decode(inp[0], is_input=True, mask=inp_mask[0]))\n",
        "                print(dataset.decode(expected_out[0], is_input=False, mask=expected_out_mask[0]))\n",
        "\n",
        "                inp = inp[0].view(1, -1)\n",
        "                inp_mask = inp_mask[0].view(1, -1)\n",
        "                # <bos> token\n",
        "                initial = torch.ones(1,1).long()\n",
        "\n",
        "                out = model_engine.module.generate(inp.to(device), initial.to(device), enc_mask=inp_mask.to(device), seq_len=len(expected_out[0]), eos_token=2)\n",
        "                print(dataset.decode(out[0], is_input=False))\n",
        "                \n",
        "                bleu(out.to(device), expected_out.to(device))\n",
        "                b = bleu.get_metric(reset=True)['BLEU']\n",
        "                print(\"BLEU metric: {}\".format(b))\n",
        "                writer.add_scalar(\"BLEU\", b, i)\n",
        "                writer.flush()\n",
        "            \n",
        "            if step > 0 and i % 1000 == 0:\n",
        "                ckpt_id = \"{}-{}-{}\".format(e + epoch, i, loss.item())\n",
        "                model_engine.save_checkpoint('drive/MyDrive/lmd_transformer/small_train/', tag=ckpt_id, client_state = {'i' : i})\n",
        "                model_engine.save_checkpoint('drive/MyDrive/lmd_transformer/small_train/', tag='latest', client_state = {'i' : i})\n",
        "            i += 1\n"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting train_performer.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8eO_LPv6GECN",
        "outputId": "0ef82b77-9fee-444e-b719-9aad6f0202ea"
      },
      "source": [
        "!deepspeed train_performer.py -df drive/MyDrive/lmd_transformer/small_dataset.parquet -v drive/MyDrive/lmd_transformer/small_ --deepspeed --deepspeed_config ds_config.json"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[2020-12-10 01:33:24,723] [WARNING] [runner.py:117:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.\n",
            "[2020-12-10 01:33:24,731] [INFO] [runner.py:355:main] cmd = /usr/bin/python3 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=29500 train_performer.py -df drive/MyDrive/lmd_transformer/small_dataset.parquet -v drive/MyDrive/lmd_transformer/small_ --deepspeed --deepspeed_config ds_config.json\n",
            "[2020-12-10 01:33:25,739] [INFO] [launch.py:71:main] 0 NCCL_VERSION 2.7.8\n",
            "[2020-12-10 01:33:25,739] [INFO] [launch.py:78:main] WORLD INFO DICT: {'localhost': [0]}\n",
            "[2020-12-10 01:33:25,739] [INFO] [launch.py:87:main] nnodes=1, num_local_procs=1, node_rank=0\n",
            "[2020-12-10 01:33:25,740] [INFO] [launch.py:99:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})\n",
            "[2020-12-10 01:33:25,740] [INFO] [launch.py:100:main] dist_world_size=1\n",
            "[2020-12-10 01:33:25,740] [INFO] [launch.py:103:main] Setting CUDA_VISIBLE_DEVICES=0\n",
            "2020-12-10 01:33:27.521784: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "[2020-12-10 01:33:43,788] [INFO] [logging.py:60:log_dist] [Rank -1] DeepSpeed info: version=0.3.8, git-hash=unknown, git-branch=unknown\n",
            "[2020-12-10 01:33:43,789] [INFO] [engine.py:148:__init__] Initializing torch distributed with backend: nccl\n",
            "[2020-12-10 01:33:43,870] [INFO] [engine.py:72:_initialize_parameter_parallel_groups] data_parallel_size: 1, parameter_parallel_size: 1\n",
            "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
            "Detected CUDA files, patching ldflags\n",
            "Emitting ninja build file /root/.cache/torch_extensions/fused_adam/build.ninja...\n",
            "Building extension module fused_adam...\n",
            "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
            "ninja: no work to do.\n",
            "Loading extension module fused_adam...\n",
            "Time to load fused_adam op: 0.5200333595275879 seconds\n",
            "[2020-12-10 01:33:45,834] [INFO] [engine.py:595:_configure_optimizer] Using DeepSpeed Optimizer param name adam as basic optimizer\n",
            "[2020-12-10 01:33:45,834] [INFO] [engine.py:598:_configure_optimizer] DeepSpeed Basic Optimizer = FusedAdam (\n",
            "Parameter Group 0\n",
            "    betas: [0.8, 0.999]\n",
            "    bias_correction: True\n",
            "    eps: 1e-08\n",
            "    lr: 0.0002\n",
            "    weight_decay: 0.0\n",
            ")\n",
            "[2020-12-10 01:33:45,834] [INFO] [engine.py:615:_configure_optimizer] Initializing AMP with these params: {'opt_level': 'O2'}\n",
            "[2020-12-10 01:33:45,834] [INFO] [engine.py:617:_configure_optimizer] Initializing Apex amp from: ['/usr/local/lib/python3.6/dist-packages/apex/amp']\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\",)\n",
            "[2020-12-10 01:33:45,925] [INFO] [engine.py:628:_configure_optimizer] DeepSpeed Final Optimizer = FusedAdam (\n",
            "Parameter Group 0\n",
            "    betas: [0.8, 0.999]\n",
            "    bias_correction: True\n",
            "    eps: 1e-08\n",
            "    lr: 0.0002\n",
            "    weight_decay: 0.0\n",
            ")\n",
            "[2020-12-10 01:33:45,925] [INFO] [engine.py:629:_configure_optimizer] DeepSpeed Final Optimizer = {'state': {}, 'param_groups': [{'lr': 0.0002, 'bias_correction': True, 'betas': [0.8, 0.999], 'eps': 1e-08, 'weight_decay': 0.0, 'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295]}]}\n",
            "[2020-12-10 01:33:45,925] [INFO] [engine.py:458:_configure_lr_scheduler] DeepSpeed using configured LR scheduler = WarmupLR\n",
            "[2020-12-10 01:33:45,925] [INFO] [logging.py:60:log_dist] [Rank 0] DeepSpeed LR Scheduler = <deepspeed.runtime.lr_schedules.WarmupLR object at 0x7f259ec0b668>\n",
            "[2020-12-10 01:33:45,925] [INFO] [logging.py:60:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0002], mom=[[0.8, 0.999]]\n",
            "[2020-12-10 01:33:45,926] [INFO] [config.py:644:print] DeepSpeedEngine configuration:\n",
            "[2020-12-10 01:33:45,926] [INFO] [config.py:648:print]   activation_checkpointing_config  <deepspeed.runtime.activation_checkpointing.config.DeepSpeedActivationCheckpointingConfig object at 0x7f267bc667b8>\n",
            "[2020-12-10 01:33:45,926] [INFO] [config.py:648:print]   allreduce_always_fp32 ........ False\n",
            "[2020-12-10 01:33:45,926] [INFO] [config.py:648:print]   amp_enabled .................. True\n",
            "[2020-12-10 01:33:45,926] [INFO] [config.py:648:print]   amp_params ................... {'opt_level': 'O2'}\n",
            "[2020-12-10 01:33:45,926] [INFO] [config.py:648:print]   disable_allgather ............ False\n",
            "[2020-12-10 01:33:45,926] [INFO] [config.py:648:print]   dump_state ................... False\n",
            "[2020-12-10 01:33:45,926] [INFO] [config.py:648:print]   dynamic_loss_scale_args ...... None\n",
            "[2020-12-10 01:33:45,926] [INFO] [config.py:648:print]   fp16_enabled ................. False\n",
            "[2020-12-10 01:33:45,926] [INFO] [config.py:648:print]   global_rank .................. 0\n",
            "[2020-12-10 01:33:45,926] [INFO] [config.py:648:print]   gradient_accumulation_steps .. 16\n",
            "[2020-12-10 01:33:45,926] [INFO] [config.py:648:print]   gradient_clipping ............ 0.0\n",
            "[2020-12-10 01:33:45,926] [INFO] [config.py:648:print]   gradient_predivide_factor .... 1.0\n",
            "[2020-12-10 01:33:45,926] [INFO] [config.py:648:print]   initial_dynamic_scale ........ 4294967296\n",
            "[2020-12-10 01:33:45,926] [INFO] [config.py:648:print]   loss_scale ................... 0\n",
            "[2020-12-10 01:33:45,926] [INFO] [config.py:648:print]   memory_breakdown ............. False\n",
            "[2020-12-10 01:33:45,926] [INFO] [config.py:648:print]   optimizer_legacy_fusion ...... False\n",
            "[2020-12-10 01:33:45,926] [INFO] [config.py:648:print]   optimizer_name ............... adam\n",
            "[2020-12-10 01:33:45,926] [INFO] [config.py:648:print]   optimizer_params ............. {'lr': 0.0002, 'betas': [0.8, 0.999], 'eps': 1e-08, 'adam_w_mode': True}\n",
            "[2020-12-10 01:33:45,926] [INFO] [config.py:648:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}\n",
            "[2020-12-10 01:33:45,926] [INFO] [config.py:648:print]   pld_enabled .................. False\n",
            "[2020-12-10 01:33:45,926] [INFO] [config.py:648:print]   pld_params ................... False\n",
            "[2020-12-10 01:33:45,927] [INFO] [config.py:648:print]   prescale_gradients ........... False\n",
            "[2020-12-10 01:33:45,927] [INFO] [config.py:648:print]   scheduler_name ............... WarmupLR\n",
            "[2020-12-10 01:33:45,927] [INFO] [config.py:648:print]   scheduler_params ............. {'warmup_min_lr': 0, 'warmup_max_lr': 0.0002, 'warmup_num_steps': 1000}\n",
            "[2020-12-10 01:33:45,927] [INFO] [config.py:648:print]   sparse_attention ............. None\n",
            "[2020-12-10 01:33:45,927] [INFO] [config.py:648:print]   sparse_gradients_enabled ..... False\n",
            "[2020-12-10 01:33:45,927] [INFO] [config.py:648:print]   steps_per_print .............. 20\n",
            "[2020-12-10 01:33:45,927] [INFO] [config.py:648:print]   tensorboard_enabled .......... False\n",
            "[2020-12-10 01:33:45,927] [INFO] [config.py:648:print]   tensorboard_job_name ......... DeepSpeedJobName\n",
            "[2020-12-10 01:33:45,927] [INFO] [config.py:648:print]   tensorboard_output_path ...... \n",
            "[2020-12-10 01:33:45,927] [INFO] [config.py:648:print]   train_batch_size ............. 32\n",
            "[2020-12-10 01:33:45,927] [INFO] [config.py:648:print]   train_micro_batch_size_per_gpu  2\n",
            "[2020-12-10 01:33:45,927] [INFO] [config.py:648:print]   wall_clock_breakdown ......... False\n",
            "[2020-12-10 01:33:45,927] [INFO] [config.py:648:print]   world_size ................... 1\n",
            "[2020-12-10 01:33:45,927] [INFO] [config.py:648:print]   zero_allow_untested_optimizer  False\n",
            "[2020-12-10 01:33:45,927] [INFO] [config.py:648:print]   zero_config .................. <deepspeed.runtime.zero.config.DeepSpeedZeroConfig object at 0x7f267bc667f0>\n",
            "[2020-12-10 01:33:45,927] [INFO] [config.py:648:print]   zero_enabled ................. False\n",
            "[2020-12-10 01:33:45,927] [INFO] [config.py:648:print]   zero_optimization_stage ...... 0\n",
            "[2020-12-10 01:33:45,927] [INFO] [config.py:655:print]   json = {\n",
            "    \"amp\":{\n",
            "        \"enabled\":true,\n",
            "        \"opt_level\":\"O2\"\n",
            "    },\n",
            "    \"gradient_accumulation_steps\":16,\n",
            "    \"max_grad_norm\":0.5,\n",
            "    \"optimizer\":{\n",
            "        \"params\":{\n",
            "            \"adam_w_mode\":true,\n",
            "            \"betas\":[\n",
            "                0.8,\n",
            "                0.999\n",
            "            ],\n",
            "            \"eps\":1e-08,\n",
            "            \"lr\":0.0002\n",
            "        },\n",
            "        \"type\":\"Adam\"\n",
            "    },\n",
            "    \"scheduler\":{\n",
            "        \"params\":{\n",
            "            \"warmup_max_lr\":0.0002,\n",
            "            \"warmup_min_lr\":0,\n",
            "            \"warmup_num_steps\":1000\n",
            "        },\n",
            "        \"type\":\"WarmupLR\"\n",
            "    },\n",
            "    \"steps_per_print\":20,\n",
            "    \"train_batch_size\":32\n",
            "}\n",
            "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
            "Emitting ninja build file /root/.cache/torch_extensions/utils/build.ninja...\n",
            "Building extension module utils...\n",
            "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
            "ninja: no work to do.\n",
            "Loading extension module utils...\n",
            "Time to load utils op: 0.5282361507415771 seconds\n",
            "Dataset maximum sequence lengths - Input: 13640, Output: 1345\n",
            "Train Dataset - size: 5471, batches: 5471\n",
            "Validate Dataset - size: 608, batches: 608\n",
            "EPOCH: 0\n",
            "training loss: 9.6875\n",
            "\n",
            " validation loss: 9.628515625 \n",
            "\n",
            "W_14,BASS_ON_35_V126,PIANO_ON_35_V82,PIANO_ON_47_V102,PIANO_ON_59_V118,PIANO_ON_62_V118,PIANO_ON_66_V118,W_2,DRUMS_ON_36_V126,DRUMS_ON_44_V122,DRUMS_ON_57_V102,W_2,DRUMS_OFF_36,DRUMS_OFF_44,DRUMS_OFF_57,W_44,PIANO_OFF_35,PIANO_OFF_47,PIANO_OFF_59,PIANO_OFF_62,PIANO_OFF_66,W_12,BASS_OFF_35,W_36,BASS_ON_35_V106,PIANO_ON_35_V90,PIANO_ON_47_V110,PIANO_ON_59_V118,PIANO_ON_62_V118,PIANO_ON_66_V102,W_2,DRUMS_ON_36_V126,DRUMS_ON_44_V74,DRUMS_ON_57_V122,W_2,DRUMS_OFF_36,DRUMS_OFF_44,DRUMS_OFF_57,W_86,PIANO_OFF_66,W_4,DRUMS_ON_38_V126,DRUMS_ON_40_V126,DRUMS_ON_44_V122,GUITAR_ON_72_V126,W_6,DRUMS_OFF_38,DRUMS_OFF_40,DRUMS_OFF_44,GUITAR_OFF_72,W_17,PIANO_OFF_59,PIANO_OFF_62,W_2,PIANO_OFF_47,W_9,PIANO_OFF_35,W_9,BASS_OFF_35,W_55,BASS_ON_31_V126,PIANO_ON_59_V90,PIANO_ON_55_V90,PIANO_ON_64_V90,PIANO_ON_43_V82,PIANO_ON_31_V86,W_2,PIANO_OFF_31,DRUMS_ON_44_V70,W_1,DRUMS_OFF_44,W_30,PIANO_OFF_43,W_10,PIANO_OFF_55,PIANO_OFF_64,W_3,PIANO_OFF_59,W_17,BASS_OFF_31,W_33,PIANO_ON_64_V110,PIANO_ON_55_V118,PIANO_ON_59_V118,PIANO_ON_43_V118,BASS_ON_31_V126,PIANO_ON_31_V90,W_1,DRUMS_ON_36_V122,DRUMS_ON_44_V122,W_3,DRUMS_OFF_36,DRUMS_OFF_44,W_91,DRUMS_ON_44_V66,W_4,DRUMS_OFF_44,W_93,PIANO_OFF_31,DRUMS_ON_38_V126,DRUMS_ON_40_V126,DRUMS_ON_44_V122,GUITAR_ON_72_V126,W_3,DRUMS_OFF_38,DRUMS_OFF_40,DRUMS_OFF_44,GUITAR_OFF_72,W_24,BASS_OFF_31,W_27,PIANO_OFF_43,W_43,PIANO_OFF_59,PIANO_ON_31_V118,BASS_ON_31_V106,PIANO_ON_62_V90,PIANO_ON_59_V114,DRUMS_ON_44_V66,W_1,DRUMS_OFF_44,W_8,PIANO_OFF_55,W_15,PIANO_OFF_64,W_28,PIANO_OFF_59,W_3,PIANO_OFF_62,W_2,BASS_OFF_31,W_38,BASS_ON_31_V106,PIANO_ON_62_V118,PIANO_ON_55_V98,PIANO_ON_59_V118,PIANO_ON_43_V118,W_2,DRUMS_ON_36_V122,DRUMS_ON_44_V122,W_2,DRUMS_OFF_36,DRUMS_OFF_44,W_91,DRUMS_ON_44_V74,W_4,DRUMS_OFF_44,W_23,PIANO_OFF_31,W_8,PIANO_OFF_43,W_1,PIANO_OFF_59,W_7,PIANO_OFF_62,PIANO_OFF_55,W_14,BASS_OFF_31,W_40,BASS_ON_33_V106,PIANO_ON_64_V78,PIANO_ON_57_V90,PIANO_ON_61_V102,PIANO_ON_33_V82,PIANO_ON_45_V118,W_2,DRUMS_ON_38_V118,DRUMS_ON_40_V118,DRUMS_ON_44_V122,GUITAR_ON_72_V126,W_3,DRUMS_OFF_38,DRUMS_OFF_40,DRUMS_OFF_44,GUITAR_OFF_72,W_89,DRUMS_ON_44_V70,W_5,DRUMS_OFF_44,W_24,PIANO_OFF_45,W_3,PIANO_OFF_33,W_10,PIANO_OFF_61,W_1,PIANO_OFF_57,W_3,PIANO_OFF_64,W_31,BASS_OFF_33,W_21,PIANO_ON_64_V118,PIANO_ON_61_V118,PIANO_ON_33_V102,PIANO_ON_45_V118,PIANO_ON_57_V110,BASS_ON_33_V106,W_2,DRUMS_ON_36_V106,DRUMS_ON_44_V122,W_3,DRUMS_OFF_36,DRUMS_OFF_44,W_90,DRUMS_ON_36_V106,DRUMS_ON_44_V66,W_5,DRUMS_OFF_36,DRUMS_OFF_44,W_59,BASS_OFF_33,W_33,BASS_ON_33_V94,DRUMS_ON_38_V126,DRUMS_ON_40_V126,DRUMS_ON_44_V122,GUITAR_ON_72_V126,W_4,DRUMS_OFF_38,DRUMS_OFF_40,DRUMS_OFF_44,GUITAR_OFF_72,W_47,BASS_OFF_33,W_44,BASS_ON_33_V94,DRUMS_ON_44_V66,W_4,DRUMS_OFF_44,W_19,PIANO_OFF_57,W_9,PIANO_OFF_33,PIANO_OFF_45,W_6,PIANO_OFF_61,W_8,PIANO_OFF_64,W_33,BASS_OFF_33,W_18,BASS_ON_35_V126,PIANO_ON_35_V90,PIANO_ON_47_V118,PIANO_ON_59_V118,PIANO_ON_62_V118,PIANO_ON_66_V90,W_3,PIANO_OFF_35,PIANO_OFF_47,PIANO_OFF_59,PIANO_OFF_62,PIANO_OFF_66,PIANO_ON_59_V98,PIANO_ON_62_V98,PIANO_ON_66_V78,DRUMS_ON_36_V126,DRUMS_ON_44_V122,DRUMS_ON_57_V90,W_3,DRUMS_OFF_36,DRUMS_OFF_44,DRUMS_OFF_57,W_41,PIANO_OFF_59,PIANO_OFF_62,PIANO_OFF_66,W_10,BASS_OFF_35,W_38,BASS_ON_35_V126,PIANO_ON_62_V118,PIANO_ON_35_V82,PIANO_ON_59_V118,PIANO_ON_47_V118,PIANO_ON_66_V90,W_2,DRUMS_ON_36_V126,DRUMS_ON_44_V74,DRUMS_ON_57_V102,W_3,DRUMS_OFF_36,DRUMS_OFF_44,DRUMS_OFF_57,W_65,PIANO_OFF_66,W_27,DRUMS_ON_38_V126,DRUMS_ON_40_V126,DRUMS_ON_44_V122,GUITAR_ON_72_V126,W_4,DRUMS_OFF_38,DRUMS_OFF_40,DRUMS_OFF_44,GUITAR_OFF_72,W_7,PIANO_OFF_47,W_3,PIANO_OFF_59,W_10,PIANO_OFF_35,W_5,PIANO_OFF_62,W_10,BASS_OFF_35,W_57,BASS_ON_31_V126,PIANO_ON_59_V82,PIANO_ON_64_V90,PIANO_ON_55_V102,PIANO_ON_43_V102,PIANO_ON_31_V82,W_2,DRUMS_ON_44_V70,W_2,DRUMS_OFF_44,W_31,PIANO_OFF_31,W_1,PIANO_OFF_43,W_7,PIANO_OFF_55,W_3,PIANO_OFF_64,W_2,PIANO_OFF_59,W_4,BASS_OFF_31,W_44,PIANO_ON_64_V102,BASS_ON_31_V106,PIANO_ON_31_V82,PIANO_ON_43_V118,PIANO_ON_55_V118,PIANO_ON_59_V118,W_3,DRUMS_ON_36_V126,DRUMS_ON_44_V122,W_1,DRUMS_OFF_36,DRUMS_OFF_44,W_91,DRUMS_ON_44_V66,W_4,DRUMS_OFF_44,W_92,PIANO_OFF_31,PIANO_OFF_43,PIANO_OFF_55,PIANO_OFF_59,W_3,DRUMS_ON_38_V126,DRUMS_ON_40_V126,DRUMS_ON_44_V122,GUITAR_ON_72_V126,W_2,DRUMS_OFF_38,DRUMS_OFF_40,DRUMS_OFF_44,GUITAR_OFF_72,W_27,BASS_OFF_31,W_25,PIANO_OFF_64,W_41,PIANO_ON_62_V54,PIANO_ON_59_V82,BASS_ON_31_V126,PIANO_ON_55_V86,PIANO_ON_31_V82,PIANO_ON_43_V78,W_2,DRUMS_ON_44_V66,W_1,DRUMS_OFF_44,W_38,PIANO_OFF_43,W_5,PIANO_OFF_31,W_3,PIANO_OFF_55,W_2,BASS_OFF_31,W_3,PIANO_OFF_59,W_2,PIANO_OFF_62,W_40,PIANO_ON_62_V118,BASS_ON_31_V106,PIANO_ON_59_V118,PIANO_ON_43_V118,PIANO_ON_55_V118,PIANO_ON_31_V110,W_1,DRUMS_ON_36_V126,DRUMS_ON_44_V122,W_2,DRUMS_OFF_36,DRUMS_OFF_44,W_91,DRUMS_ON_44_V74,W_4,DRUMS_OFF_44,W_29,PIANO_OFF_31,W_4,PIANO_OFF_43,PIANO_OFF_55,W_12,PIANO_OFF_59,W_11,PIANO_OFF_62,BASS_OFF_31,W_37,BASS_ON_33_V106,PIANO_ON_64_V74,PIANO_ON_61_V118,PIANO_ON_45_V118,PIANO_ON_57_V102,PIANO_ON_33_V90,W_1,DRUMS_ON_38_V126,DRUMS_ON_40_V126,DRUMS_ON_44_V122,GUITAR_ON_72_V126,W_4,DRUMS_OFF_38,DRUMS_OFF_40,DRUMS_OFF_44,GUITAR_OFF_72,W_90,DRUMS_ON_44_V70,W_4,DRUMS_OFF_44,W_27,PIANO_OFF_33,W_2,PIANO_OFF_57,W_5,PIANO_OFF_45,W_9,PIANO_OFF_61,W_2,PIANO_OFF_64,W_9,BASS_OFF_33,W_39,PIANO_ON_59_V110,PIANO_ON_62_V110,PIANO_ON_66_V118,PIANO_ON_47_V118,PIANO_ON_35_V78,BASS_ON_35_V126,W_2,DRUMS_ON_36_V126,DRUMS_ON_44_V122,W_1,DRUMS_OFF_36,DRUMS_OFF_44,W_92,DRUMS_ON_36_V126,DRUMS_ON_44_V66,W_4,DRUMS_OFF_36,DRUMS_OFF_44,W_71,BASS_OFF_35,W_22,BASS_ON_30_V126,DRUMS_ON_38_V126,DRUMS_ON_40_V126,DRUMS_ON_44_V122,GUITAR_ON_72_V126,W_4,DRUMS_OFF_38,DRUMS_OFF_40,DRUMS_OFF_44,GUITAR_OFF_72,W_43,PIANO_OFF_35,W_19,PIANO_OFF_47,BASS_OFF_30,W_24,BASS_ON_33_V106,DRUMS_ON_44_V66,PIANO_ON_35_V102,W_2,PIANO_OFF_35,W_4,DRUMS_OFF_44,W_42,PIANO_OFF_62,PIANO_OFF_66,W_3,PIANO_OFF_59,W_29,BASS_OFF_33,W_23,BASS_ON_35_V126,PIANO_ON_59_V102,PIANO_ON_47_V118,PIANO_ON_62_V110,PIANO_ON_66_V118,W_2,PIANO_OFF_47,PIANO_OFF_62,PIANO_OFF_66,PIANO_ON_62_V122,PIANO_ON_66_V114,DRUMS_ON_36_V126,DRUMS_ON_44_V122,DRUMS_ON_57_V90,W_1,DRUMS_OFF_36,DRUMS_OFF_44,DRUMS_OFF_57,W_44,PIANO_OFF_59,PIANO_OFF_62,PIANO_OFF_66,W_4,BASS_OFF_35,W_44,PIANO_ON_35_V62,BASS_ON_35_V126,PIANO_ON_62_V110,PIANO_ON_47_V118,PIANO_ON_59_V118,PIANO_ON_66_V118,W_1,DRUMS_ON_36_V126,DRUMS_ON_44_V74,DRUMS_ON_57_V122,W_3,DRUMS_OFF_36,DRUMS_OFF_44,DRUMS_OFF_57,W_92,DRUMS_ON_38_V126,DRUMS_ON_40_V126,DRUMS_ON_44_V122,GUITAR_ON_72_V126,W_3,DRUMS_OFF_38,DRUMS_OFF_40,DRUMS_OFF_44,GUITAR_OFF_72,W_24,PIANO_OFF_66,W_2,PIANO_OFF_47,PIANO_OFF_59,W_8,PIANO_OFF_62,W_2,BASS_OFF_35,W_7,PIANO_OFF_35,W_50,BASS_ON_31_V126,PIANO_ON_31_V86,PIANO_ON_43_V74,PIANO_ON_55_V78,PIANO_ON_59_V102,PIANO_ON_64_V82,W_1,PIANO_OFF_31,PIANO_OFF_43,PIANO_OFF_55,PIANO_OFF_59,PIANO_OFF_64,PIANO_ON_59_V86,PIANO_ON_64_V66,PIANO_ON_55_V46,DRUMS_ON_44_V70,W_3,DRUMS_OFF_44,W_24,PIANO_OFF_55,W_14,PIANO_OFF_64,W_7,PIANO_OFF_59,W_7,BASS_OFF_31,W_40,BASS_ON_31_V126,PIANO_ON_64_V90,PIANO_ON_59_V118,PIANO_ON_43_V118,PIANO_ON_55_V118,PIANO_ON_31_V90,W_2,DRUMS_ON_36_V126,DRUMS_ON_44_V122,W_3,DRUMS_OFF_36,DRUMS_OFF_44,W_90,DRUMS_ON_44_V66,W_4,DRUMS_OFF_44,W_93,PIANO_OFF_31,DRUMS_ON_38_V126,DRUMS_ON_40_V126,GUITAR_ON_72_V126,W_1,DRUMS_ON_44_V122,W_3,DRUMS_OFF_38,DRUMS_OFF_40,DRUMS_OFF_44,GUITAR_OFF_72,W_27,PIANO_OFF_55,W_3,PIANO_OFF_43,W_4,PIANO_OFF_59,W_12,PIANO_OFF_64,W_5,BASS_OFF_31,W_41,BASS_ON_31_V106,PIANO_ON_31_V62,PIANO_ON_43_V78,PIANO_ON_55_V90,PIANO_ON_59_V110,PIANO_ON_62_V82,W_2,PIANO_OFF_43,PIANO_OFF_55,PIANO_OFF_59,PIANO_OFF_62,PIANO_ON_62_V70,PIANO_ON_55_V86,PIANO_ON_59_V94,DRUMS_ON_44_V66,W_2,DRUMS_OFF_44,W_34,PIANO_OFF_59,W_2,PIANO_OFF_55,W_2,PIANO_OFF_62,W_6,PIANO_OFF_31,W_6,BASS_OFF_31,W_42,BASS_ON_31_V126,PIANO_ON_62_V118,PIANO_ON_59_V118,PIANO_ON_43_V118,PIANO_ON_31_V82,PIANO_ON_55_V118,W_2,DRUMS_ON_36_V126,DRUMS_ON_44_V122,W_1,DRUMS_OFF_36,DRUMS_OFF_44,W_92,DRUMS_ON_44_V74,W_4,DRUMS_OFF_44,W_22,PIANO_OFF_31,PIANO_OFF_55,W_4,PIANO_OFF_43,W_19,PIANO_OFF_59,W_6,PIANO_OFF_62,W_5,BASS_OFF_31,W_37,BASS_ON_33_V106,PIANO_ON_33_V90,PIANO_ON_45_V118,PIANO_ON_57_V90,PIANO_ON_61_V118,PIANO_ON_64_V82,W_2,DRUMS_ON_38_V126,DRUMS_ON_40_V126,DRUMS_ON_44_V122,GUITAR_ON_72_V126,W_2,DRUMS_OFF_38,DRUMS_OFF_40,DRUMS_OFF_44,GUITAR_OFF_72,W_91,DRUMS_ON_44_V70,W_3,DRUMS_OFF_44,W_29,PIANO_OFF_33,PIANO_OFF_45,PIANO_OFF_57,PIANO_OFF_61,PIANO_OFF_64,W_43,BASS_OFF_33,W_22,PIANO_ON_64_V110,PIANO_ON_61_V118,PIANO_ON_57_V118,PIANO_ON_45_V118,PIANO_ON_33_V102,BASS_ON_33_V94,W_2,DRUMS_ON_36_V126,DRUMS_ON_44_V122,W_2,DRUMS_OFF_36,DRUMS_OFF_44,W_93,DRUMS_ON_36_V126,DRUMS_ON_44_V66,W_3,DRUMS_OFF_36,DRUMS_OFF_44,W_48,BASS_OFF_33,W_44,BASS_ON_33_V82,DRUMS_ON_38_V126,DRUMS_ON_40_V126,DRUMS_ON_44_V122,GUITAR_ON_72_V126,W_4,DRUMS_OFF_38,DRUMS_OFF_40,DRUMS_OFF_44,GUITAR_OFF_72,W_51,BASS_OFF_33,W_40,BASS_ON_33_V94,DRUMS_ON_44_V66,W_4,DRUMS_OFF_44,W_39,PIANO_OFF_33,W_1,PIANO_OFF_45,W_6,PIANO_OFF_57,W_14,PIANO_OFF_61,W_4,PIANO_OFF_64,W_17,BASS_OFF_33,W_12,BASS_ON_35_V126,PIANO_ON_47_V118,PIANO_ON_59_V118,PIANO_ON_62_V110,PIANO_ON_66_V90,PIANO_ON_35_V62,W_2,PIANO_OFF_35,DRUMS_ON_36_V126,DRUMS_ON_44_V122,DRUMS_ON_57_V46,W_2,PIANO_OFF_47,PIANO_OFF_59,PIANO_OFF_62,PIANO_OFF_66,DRUMS_OFF_36,DRUMS_OFF_44,DRUMS_OFF_57,W_55,BASS_OFF_35,W_37,BASS_ON_35_V126,PIANO_ON_62_V118,PIANO_ON_35_V90,PIANO_ON_59_V118,PIANO_ON_47_V118,PIANO_ON_66_V98,W_1,DRUMS_ON_38_V126,DRUMS_ON_40_V126,DRUMS_ON_44_V74,DRUMS_ON_57_V78,W_4,DRUMS_OFF_38,DRUMS_OFF_40,DRUMS_OFF_44,DRUMS_OFF_57,W_90,DRUMS_ON_44_V122,GUITAR_ON_72_V126,W_4,DRUMS_OFF_44,GUITAR_OFF_72,W_5,PIANO_OFF_66,W_7,PIANO_OFF_47,W_15,PIANO_OFF_59,W_1,PIANO_OFF_35,W_9,PIANO_OFF_62,W_6,BASS_OFF_35,W_50,BASS_ON_31_V106,PIANO_ON_59_V78,PIANO_ON_43_V46,PIANO_ON_55_V82,PIANO_ON_64_V78,PIANO_ON_31_V82,W_2,DRUMS_ON_36_V106,DRUMS_ON_44_V70,DRUMS_ON_57_V102,W_4,DRUMS_OFF_36,DRUMS_OFF_44,DRUMS_OFF_57,W_26,PIANO_OFF_31,W_4,PIANO_OFF_64,W_2,PIANO_OFF_55,W_1,PIANO_OFF_43,W_1,PIANO_OFF_59,W_27,BASS_OFF_31,W_29,PIANO_ON_64_V118,BASS_ON_31_V126,PIANO_ON_31_V82,PIANO_ON_43_V118,PIANO_ON_55_V118,PIANO_ON_59_V118,W_3,DRUMS_ON_36_V126,DRUMS_ON_44_V122,DRUMS_ON_57_V122,W_1,DRUMS_OFF_36,DRUMS_OFF_44,DRUMS_OFF_57,W_91,DRUMS_ON_44_V66,W_4,DRUMS_OFF_44,W_92,PIANO_OFF_31,PIANO_OFF_43,PIANO_OFF_55,PIANO_OFF_59,W_1,DRUMS_ON_38_V126,DRUMS_ON_40_V126,DRUMS_ON_44_V122,GUITAR_ON_72_V126,W_4,DRUMS_OFF_38,DRUMS_OFF_40,DRUMS_OFF_44,GUITAR_OFF_72,W_27,BASS_OFF_31,W_16,PIANO_OFF_64,W_49,BASS_ON_31_V106,PIANO_ON_62_V82,PIANO_ON_31_V74,PIANO_ON_59_V118,PIANO_ON_55_V90,PIANO_ON_43_V78,W_1,DRUMS_ON_36_V118,DRUMS_ON_44_V66,W_2,DRUMS_OFF_36,DRUMS_OFF_44,W_33,PIANO_OFF_43,W_1,PIANO_OFF_55,W_2,PIANO_OFF_31,PIANO_OFF_59,W_2,PIANO_OFF_62,W_12,BASS_OFF_31,W_43,BASS_ON_31_V126,PIANO_ON_62_V110,PIANO_ON_59_V118,PIANO_ON_55_V102,PIANO_ON_43_V90,PIANO_ON_31_V82,W_2,DRUMS_ON_36_V126,DRUMS_ON_44_V122,W_3,DRUMS_OFF_36,DRUMS_OFF_44,W_90,DRUMS_ON_44_V66,W_4,DRUMS_OFF_44,W_21,PIANO_OFF_31,W_1,PIANO_OFF_43,W_3,PIANO_OFF_55,W_16,PIANO_OFF_59,W_7,PIANO_OFF_62,W_18,BASS_OFF_31,W_9,BASS_ON_33_V94,PIANO_ON_61_V118,PIANO_ON_64_V90,PIANO_ON_45_V118,PIANO_ON_33_V82,PIANO_ON_57_V102,W_4,DRUMS_ON_38_V126,DRUMS_ON_40_V126,DRUMS_ON_44_V122,GUITAR_ON_72_V126,W_4,DRUMS_OFF_38,DRUMS_OFF_40,DRUMS_OFF_44,GUITAR_OFF_72,W_127,PIANO_OFF_57,W_8,PIANO_OFF_33,W_2,PIANO_OFF_45,W_3,PIANO_OFF_61,PIANO_OFF_64,W_26,BASS_OFF_33,W_36,PIANO_ON_35_V82,PIANO_ON_47_V86,PIANO_ON_62_V86,PIANO_ON_66_V86,PIANO_ON_59_V86,BASS_ON_35_V126,W_2,DRUMS_ON_36_V106,DRUMS_ON_44_V122,W_1,DRUMS_OFF_36,DRUMS_OFF_44,W_92,DRUMS_ON_46_V122,W_4,DRUMS_OFF_46,W_80,BASS_OFF_35,W_13,BASS_ON_30_V126,DRUMS_ON_44_V122,W_3,PIANO_OFF_59,DRUMS_OFF_44,W_36,PIANO_OFF_66,W_4,PIANO_OFF_62,W_52,DRUMS_ON_38_V126,DRUMS_ON_40_V126,W_4,DRUMS_OFF_38,DRUMS_OFF_40,W_77,BASS_OFF_30,W_10,PIANO_OFF_47,W_6,PIANO_ON_59_V62,PIANO_ON_66_V66,PIANO_ON_73_V78,DRUMS_ON_36_V102,DRUMS_ON_44_V94,W_2,PIANO_OFF_35,BASS_ON_35_V102,W_2,DRUMS_OFF_36,DRUMS_OFF_44,W_80,PIANO_OFF_73,W_11,PIANO_ON_74_V70,W_89,PIANO_OFF_74,W_8,PIANO_ON_71_V58,DRUMS_ON_37_V122,DRUMS_ON_44_V94,DRUMS_ON_60_V82,W_3,DRUMS_OFF_37,DRUMS_OFF_44,DRUMS_OFF_60,W_21,PIANO_OFF_66,W_21,PIANO_OFF_59,W_38,PIANO_OFF_71,W_3,BASS_OFF_35,W_10,BASS_ON_31_V90,PIANO_ON_55_V90,PIANO_ON_62_V90,PIANO_ON_74_V90,DRUMS_ON_36_V102,W_3,DRUMS_OFF_36,W_79,PIANO_OFF_74,W_14,PIANO_ON_73_V98,DRUMS_ON_36_V122,DRUMS_ON_44_V94,W_3,DRUMS_OFF_36,DRUMS_OFF_44,W_89,PIANO_OFF_73,W_3,PIANO_ON_74_V86,DRUMS_ON_46_V74,W_4,DRUMS_OFF_46,W_88,PIANO_OFF_74,W_5,PIANO_ON_71_V62,DRUMS_ON_37_V122,DRUMS_ON_44_V94,DRUMS_ON_60_V82,W_3,DRUMS_OFF_37,DRUMS_OFF_44,DRUMS_OFF_60,W_86,PIANO_OFF_71,W_6,PIANO_ON_74_V98,W_57,PIANO_OFF_62,W_13,PIANO_OFF_55,W_20,PIANO_OFF_74,W_4,BASS_OFF_31,W_3,BASS_ON_33_V102,PIANO_ON_57_V74,PIANO_ON_64_V74,PIANO_ON_73_V106,DRUMS_ON_36_V102,DRUMS_ON_44_V94,W_3,DRUMS_OFF_36,DRUMS_OFF_44,W_91,PIANO_OFF_73,W_1,PIANO_ON_74_V74,W_94,PIANO_OFF_74,W_3,PIANO_ON_71_V74,DRUMS_ON_37_V122,DRUMS_ON_44_V94,DRUMS_ON_60_V82,W_3,DRUMS_OFF_37,DRUMS_OFF_44,DRUMS_OFF_60,W_27,PIANO_OFF_64,W_16,PIANO_OFF_57,W_40,PIANO_OFF_71,W_10,PIANO_ON_59_V50,PIANO_ON_66_V54,PIANO_ON_74_V86,DRUMS_ON_36_V102,W_3,DRUMS_OFF_36,W_69,PIANO_OFF_74,W_25,BASS_ON_35_V82,PIANO_ON_73_V46,DRUMS_ON_36_V122,DRUMS_ON_44_V94,W_2,DRUMS_OFF_36,DRUMS_OFF_44,W_3,BASS_OFF_33,W_82,PIANO_OFF_73,W_7,PIANO_ON_74_V58,DRUMS_ON_46_V74,W_4,DRUMS_OFF_46,W_93,PIANO_ON_76_V62,DRUMS_ON_37_V122,DRUMS_ON_44_V94,DRUMS_ON_60_V82,W_2,PIANO_OFF_74,W_1,DRUMS_OFF_37,DRUMS_OFF_44,DRUMS_OFF_60,W_91,PIANO_OFF_76,W_1,PIANO_ON_74_V58,W_23,PIANO_OFF_66,W_23,PIANO_OFF_59,W_9,BASS_OFF_35,W_18,PIANO_OFF_74,W_24,BASS_ON_35_V122,PIANO_ON_59_V50,PIANO_ON_66_V70,PIANO_ON_73_V98,DRUMS_ON_36_V102,DRUMS_ON_44_V94,W_2,DRUMS_OFF_36,DRUMS_OFF_44,W_93,PIANO_ON_74_V86,W_3,PIANO_OFF_73,W_89,PIANO_OFF_74,W_5,PIANO_ON_71_V50,DRUMS_ON_37_V122,DRUMS_ON_44_V94,DRUMS_ON_60_V82,W_3,DRUMS_OFF_37,DRUMS_OFF_44,DRUMS_OFF_60,W_10,PIANO_OFF_66,W_37,PIANO_OFF_59,W_30,PIANO_OFF_71,W_16,BASS_ON_31_V94,PIANO_ON_55_V62,PIANO_ON_62_V62,PIANO_ON_74_V86,DRUMS_ON_36_V102,W_4,DRUMS_OFF_36,BASS_OFF_35,W_89,PIANO_OFF_74,W_3,PIANO_ON_73_V70,DRUMS_ON_36_V122,DRUMS_ON_44_V94,W_2,DRUMS_OFF_36,DRUMS_OFF_44,W_77,PIANO_OFF_73,W_16,PIANO_ON_74_V74,DRUMS_ON_46_V74,W_4,DRUMS_OFF_46,W_83,PIANO_OFF_74,W_10,PIANO_ON_71_V70,DRUMS_ON_37_V122,DRUMS_ON_44_V94,DRUMS_ON_60_V82,W_4,DRUMS_OFF_37,DRUMS_OFF_44,DRUMS_OFF_60,W_86,PIANO_OFF_71,W_4,PIANO_ON_74_V98,W_48,PIANO_OFF_62,W_24,PIANO_OFF_55,W_15,PIANO_OFF_74,W_11,BASS_OFF_31,BASS_ON_33_V114,PIANO_ON_57_V70,PIANO_ON_64_V74,PIANO_ON_73_V106,DRUMS_ON_36_V102,W_2,DRUMS_ON_44_V94,W_1,DRUMS_OFF_36,DRUMS_OFF_44,W_92,PIANO_ON_74_V98,W_4,PIANO_OFF_73,W_84,PIANO_OFF_74,W_9,PIANO_ON_73_V54,DRUMS_ON_37_V122,DRUMS_ON_44_V94,DRUMS_ON_60_V82,W_3,DRUMS_OFF_37,DRUMS_OFF_44,DRUMS_OFF_60,W_26,PIANO_OFF_64,W_18,PIANO_OFF_57,W_45,PIANO_OFF_73,W_4,PIANO_ON_59_V82,PIANO_ON_66_V82,PIANO_ON_73_V90,DRUMS_ON_36_V102,W_3,DRUMS_OFF_36,W_93,BASS_ON_35_V78,DRUMS_ON_36_V122,DRUMS_ON_44_V94,PIANO_ON_74_V106,W_1,PIANO_OFF_74,W_3,DRUMS_OFF_36,DRUMS_OFF_44,W_3,BASS_OFF_33,W_85,PIANO_OFF_73,W_3,PIANO_ON_73_V122,DRUMS_ON_46_V74,W_4,DRUMS_OFF_46,W_93,PIANO_ON_71_V122,DRUMS_ON_37_V122,DRUMS_ON_44_V94,DRUMS_ON_60_V82,W_4,DRUMS_OFF_37,DRUMS_OFF_44,DRUMS_OFF_60,W_7,PIANO_OFF_73,W_81,PIANO_OFF_71,W_3,PIANO_ON_73_V98,W_24,PIANO_OFF_66,W_17,BASS_OFF_35,W_6,PIANO_OFF_59,W_51,PIANO_OFF_73,BASS_ON_35_V122,PIANO_ON_59_V54,PIANO_ON_66_V58,PIANO_ON_73_V78,DRUMS_ON_36_V102,DRUMS_ON_44_V94,W_1,DRUMS_OFF_36,DRUMS_OFF_44,W_92,PIANO_OFF_73,W_2,PIANO_ON_74_V62,W_89,PIANO_OFF_74,W_7,PIANO_ON_71_V58,DRUMS_ON_37_V122,DRUMS_ON_44_V94,DRUMS_ON_60_V82,W_3,DRUMS_OFF_37,DRUMS_OFF_44,DRUMS_OFF_60,W_2,PIANO_OFF_66,W_35,PIANO_OFF_59,W_34,PIANO_OFF_71,W_22,BASS_ON_31_V94,PIANO_ON_55_V58,PIANO_ON_62_V58,PIANO_ON_74_V98,DRUMS_ON_36_V102,W_3,DRUMS_OFF_36,W_10,BASS_OFF_35,W_58,PIANO_OFF_74,W_25,PIANO_ON_73_V42,DRUMS_ON_36_V122,DRUMS_ON_44_V94,W_3,DRUMS_OFF_36,DRUMS_OFF_44,W_82,PIANO_OFF_73,W_11,PIANO_ON_74_V70,DRUMS_ON_46_V74,W_3,DRUMS_OFF_46,W_81,PIANO_OFF_74,W_11,PIANO_ON_71_V50,DRUMS_ON_37_V122,DRUMS_ON_44_V94,DRUMS_ON_60_V82,W_4,DRUMS_OFF_37,DRUMS_OFF_44,DRUMS_OFF_60,W_80,PIANO_OFF_71,W_12,PIANO_ON_74_V54,W_49,PIANO_OFF_62,W_16,PIANO_OFF_55,W_32,PIANO_OFF_74,BASS_ON_33_V102,PIANO_ON_57_V58,PIANO_ON_64_V58,PIANO_ON_73_V70,DRUMS_ON_36_V102,DRUMS_ON_44_V94,W_4,DRUMS_OFF_36,DRUMS_OFF_44,W_7,BASS_OFF_31,W_78,PIANO_OFF_73,W_6,PIANO_ON_74_V62,W_92,PIANO_OFF_74,W_4,PIANO_ON_71_V58,DRUMS_ON_37_V122,DRUMS_ON_44_V94,DRUMS_ON_60_V82,W_4,DRUMS_OFF_37,DRUMS_OFF_44,DRUMS_OFF_60,W_31,PIANO_OFF_64,W_9,PIANO_OFF_57,W_38,PIANO_OFF_71,W_15,PIANO_ON_59_V70,PIANO_ON_66_V54,BASS_ON_35_V106,PIANO_ON_74_V86,DRUMS_ON_36_V102,W_3,DRUMS_OFF_36,W_3,BASS_OFF_33,W_81,PIANO_OFF_74,W_9,PIANO_ON_71_V90,DRUMS_ON_36_V122,DRUMS_ON_44_V94,W_4,DRUMS_OFF_36,DRUMS_OFF_44,W_82,PIANO_OFF_71,W_9,PIANO_ON_74_V78,DRUMS_ON_46_V74,W_4,DRUMS_OFF_46,W_93,PIANO_OFF_74,PIANO_ON_78_V74,DRUMS_ON_37_V122,DRUMS_ON_44_V94,DRUMS_ON_60_V82,W_3,DRUMS_OFF_37,DRUMS_OFF_44,DRUMS_OFF_60,W_87,PIANO_OFF_78,W_5,PIANO_ON_71_V86,W_3,BASS_OFF_35,W_24,PIANO_OFF_66,W_14,PIANO_OFF_59,W_42,PIANO_OFF_71,W_14,BASS_ON_35_V102,PIANO_ON_59_V54,PIANO_ON_66_V70,PIANO_ON_83_V102,DRUMS_ON_36_V102,DRUMS_ON_44_V94,W_4,DRUMS_OFF_36,DRUMS_OFF_44,W_80,PIANO_OFF_83,W_10,PIANO_ON_78_V82,W_85,PIANO_OFF_78,W_13,PIANO_ON_74_V74,DRUMS_ON_37_V122,DRUMS_ON_44_V94,DRUMS_ON_60_V82,W_4,DRUMS_OFF_37,DRUMS_OFF_44,DRUMS_OFF_60,W_24,PIANO_OFF_66,W_41,PIANO_OFF_59,W_14,PIANO_OFF_74,W_13,BASS_ON_31_V86,PIANO_ON_55_V70,PIANO_ON_62_V70,PIANO_ON_71_V74,DRUMS_ON_36_V102,W_3,DRUMS_OFF_36,W_1,BASS_OFF_35,W_76,PIANO_OFF_71,W_16,PIANO_ON_83_V98,DRUMS_ON_36_V122,DRUMS_ON_44_V94,W_3,DRUMS_OFF_36,DRUMS_OFF_44,W_86,PIANO_OFF_83,W_6,PIANO_ON_79_V82,DRUMS_ON_46_V74,W_4,DRUMS_OFF_46,W_82,PIANO_OFF_79,W_10,PIANO_ON_74_V82,DRUMS_ON_37_V122,DRUMS_ON_44_V94,DRUMS_ON_60_V82,W_4,DRUMS_OFF_37,DRUMS_OFF_44,DRUMS_OFF_60,W_79,PIANO_OFF_74,W_13,PIANO_ON_71_V70,W_2,PIANO_OFF_62,W_47,PIANO_OFF_55,W_43,PIANO_OFF_71,W_5,DRUMS_ON_36_V102,DRUMS_ON_44_V94,W_3,DRUMS_OFF_36,DRUMS_OFF_44,W_14,BASS_OFF_31,<eos>\n",
            "<bos>,W_9,BE,ON_66_V102,W_36,OFF_66,W_65,CAUSE ,ON_66_V102,W_148,OFF_66,W_40,THE ,ON_64_V102,W_38,OFF_64,W_62,NIGHT ,ON_64_V102,W_289,OFF_64,BE,ON_62_V102,W_41,OFF_62,W_57,LONGS ,ON_62_V102,W_150,OFF_62,W_39,TO ,ON_64_V102,W_78,OFF_64,W_113,LO,ON_64_V102,W_85,OFF_64,W_12,VERS ,ON_66_V102,W_223,OFF_66,W_65,BE,ON_66_V102,W_27,OFF_66,W_70,CAUSE ,ON_66_V102,W_163,OFF_66,W_26,THE ,ON_64_V102,W_48,OFF_64,W_50,NIGHT ,ON_64_V102,W_271,OFF_64,W_18,BE,ON_62_V102,W_34,OFF_62,W_62,LONGS ,ON_62_V102,W_156,OFF_62,W_33,TO ,ON_64_V102,W_98,OFF_64,W_94,LI,ON_64_V102,W_96,OFF_64,FE ,ON_62_V102,W_256,OFF_62,W_33,BE,ON_66_V102,W_81,OFF_66,W_18,CAUSE ,ON_66_V102,W_155,OFF_66,W_32,THE ,ON_64_V102,W_77,OFF_64,W_23,NIGHT ,ON_64_V102,W_263,OFF_64,W_21,BE,ON_62_V102,W_40,OFF_62,W_58,LONGS ,ON_62_V102,W_160,OFF_62,W_33,TO ,ON_64_V102,W_76,OFF_64,W_114,LO,ON_64_V102,W_86,OFF_64,W_12,VERS ,ON_66_V102,W_230,OFF_66,W_59,BE,ON_66_V102,W_83,OFF_66,W_16,CAUSE ,ON_66_V102,W_156,OFF_66,W_33,THE ,ON_64_V102,W_71,OFF_64,W_27,NIGHT ,ON_64_V102,W_258,OFF_64,W_27,BE,ON_62_V102,W_29,OFF_62,W_69,LONGS ,ON_62_V102,W_153,OFF_62,W_19,TO ,ON_64_V102,W_92,OFF_64,W_115,LI,ON_64_V102,W_95,OFF_64,W_4,FE ,ON_62_V102,W_254,OFF_62,W_36,IF ,ON_59_V102,W_31,OFF_59,W_67,I ,ON_59_V102,W_126,OFF_59,W_60,DE,ON_62_V102,W_83,OFF_62,W_16,AL,ON_59_V102,W_249,OFF_59,W_35,WHEN ,ON_59_V102,W_99,OFF_59,I'M ,ON_57_V102,W_143,OFF_57,W_47,A,ON_57_V102,W_121,OFF_57,W_70,LO,ON_57_V102,W_81,OFF_57,W_16,NE ,ON_59_V102,W_233,OFF_59,W_55,LOVE ,ON_59_V102,W_49,OFF_59,W_52,THE ,ON_59_V102,W_109,OFF_59,W_82,RING ,ON_62_V102,W_175,OFF_62,W_15,OF ,ON_59_V102,W_143,OFF_59,W_51,THE ,ON_59_V102,W_69,OFF_59,W_26,TE,ON_57_V102,W_171,OFF_57,W_20,LE,ON_59_V102,W_141,OFF_59,W_52,PHONE ,ON_59_V102,W_314,OFF_59,W_72,LOVE ,ON_59_V102,W_22,OFF_59,W_74,AN ,ON_59_V102,W_129,OFF_59,W_62,AN,ON_62_V102,W_175,OFF_62,W_16,GEL ,ON_59_V102,W_70,OFF_59,W_118,DIS,ON_59_V102,W_73,OFF_59,W_23,GUISED ,ON_57_V102,W_167,OFF_57,W_28,AS ,ON_57_V102,W_104,OFF_57,W_86,STA,ON_57_V102,W_85,OFF_57,W_14,RS ,ON_59_V102,W_230,OFF_59,W_58,IN ,ON_59_V102,W_24,OFF_59,W_71,OUR ,ON_59_V102,W_119,OFF_59,W_73,UN,ON_62_V102,W_98,OFF_62,W_3,TIL ,ON_59_V102,W_207,OFF_59,W_73,THE ,ON_59_V102,W_85,OFF_59,<eos>\n",
            "HOW,ON_32_V118,MEN ,SOU,LAX,'TIL ,ON_68_V54,W_707,AH ,CHICK,ON_67_V126,ERNED ,EM ,ON_34_V26,SUMMER,ON_82_V34,ON_103_V102,PITE ,W_292,TOUGH,ON_106_V94,VIVIR ,NAILS ,CEAN'S ,CHINE ,HING ,ERE ,ON_101_V74,JOHNN,ON_93_V86,W_894,ON_80_V86,PLAYS ,W_301,OS,W_962,ON_34_V42,W_4,W_206,W_101,W_939,TIGHT ,W_156,W_343,FLOAT ,EEH,YO,WHEN,ON_48_V50,W_143,TLY ,ON_23_V14,ON_106_V14,ON_81_V22,ON_101_V70,ON_70_V110,W_371,SET ,ON_32_V10,IN THE ,W_130,LEMS ,ON_61_V38,HUNT,ON_98_V110,KERS ,W_103,ON_21_V114,ON_49_V118,W_1000,STRUT ,W_719,RYE ,ON_43_V66,W_256,ON_79_V70,RIOUS ,ON_46_V102,W_289,SHO ,',SALE ,COW,FACTS ,SCA,ON_71_V122,ON_96_V66,ON_76_V30,ON_69_V34,ON_28_V22,FARE ,ON_99_V62,STAKES ,PRAYING ,ON_38_V6,ON_56_V66,EVE,W_417,ON_99_V34,OUTSIDE ,ON_55_V58,OC,ON_100_V110,ON_29_V26,THE ,W_827,ON_39_V122,TAS,PHI,ON_30_V46,UPS ,ON_47_V118,CKET ,JOAN ,ON_100_V62,FEED ,DIN,W_407,SIP ,ON_97_V62,W_322,TI,BACKS ,LENCE ,ON_31_V74,W_394,FACING ,LDS ,L ,FIGHT ,ON_61_V58,W_466,ON_32_V102,W_677,ON_103_V6,ON_35_V38,ON_108_V42,ON_26_V82,ON_56_V2,W_95,ON_26_V82,ON_91_V38,ON_70_V118,HID,OFF_43,ON_62_V90,ON_39_V90,STAY,W_426,CRITI,ON_25_V126,OK ,ORS ,ON_66_V6,W_47,WOW ,ON_42_V6,STOOD ,W_636,WOE IS ,ON_94_V126,CHOKE ,W_879,ON_94_V106,W_908,ON_35_V50,L'I ,ROUND ,LUNCH,ON_51_V110,W_691,THEN ,I,SUFFRAGETTE ,ON_53_V90,PAS ,SPEED ,CLAI,BACKS ,ON_55_V82,NOR,ON_105_V6,VEYOU ,BULB'S ,TALKED ,W_438,TORN ,ON_94_V62,JOK,PREME ,ON_87_V90,ON_69_V2,ON_49_V114,ON_91_V82,ON_99_V126,W_450,GUESS ,W_411,NECKS ,MIM ,TRIVIALITY ,ON_80_V66,ON_73_V58,PHONE ,W_591,EVERYTHING ,ON_76_V10,W_363,ON_21_V70,GREAT,ON_41_V14,ZI ,W_181,W_680,ON_87_V30,AMONG ,W_268,W_585,CUE ,ON_103_V70,ON_45_V2,W_278,ON_36_V118,NOL,ON_106_V90,BUSI,ON_52_V38,EAH ,W_795,ON_94_V38,FA,W_736,ON_28_V90,EDG,SIN,W_423,W_116,BOO ,W_75,STAYED ,OF MANY ,FOLL,ON_27_V102,HOSTS ,ON_44_V86,YOUR'S ,ON_27_V102,ON_68_V74,ON_67_V106,BRO ,TUN,W_498,LAND'S ,W_286,W_466,ON_82_V10,STO IN,BIES ,ON_78_V122,ON_69_V2,W_109,CREDIT ,SOY ,BUST,W_790,BLAMED ,RRA ,WILD,WELCOME ,CHIEVE ,OFF_36,BEATS ,GIVES ,SUPER ,ON_70_V118,PLEASE ,CLINCH ,ON_85_V126,TLY ,MH ,W_265,ON_88_V66,W_76,GLECT ,RUMP ,GRO ,W_680,FORE,ON_77_V78,ON_44_V10,ON_45_V42,ON_31_V14,ON_63_V82,DREA,W_571,PORCH ,DE,W_467,W_294,ON_71_V106,W_5,CHICK,NIOR'S ,ELSE,TS ,ON_76_V30,ON_40_V114,RANG,W_361,W_488,ON_42_V62,W_436,W_982,BERED ,ON_52_V78,ON_90_V114,NAL,ON_61_V70,NE ,BOP ,PRE ,PIU ,CRYSTAL ,W_597,ON_77_V58,ON_49_V66,TECTS ,W_1000,SELVES ,ON_85_V46,W_771,ON_22_V26,ON_68_V22,PRI\n",
            "BLEU metric: 1.0155390870943748e-12\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "[2020-12-10 01:35:59,614] [INFO] [timer.py:166:stop] 0/20, SamplesPerSec=0.3943798550063277\n",
            "training loss: 9.619140625\n",
            "[2020-12-10 01:37:48,011] [INFO] [timer.py:166:stop] 0/40, SamplesPerSec=0.3806151448317544\n",
            "training loss: 9.612890625\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "[2020-12-10 01:39:59,973] [INFO] [timer.py:166:stop] 0/60, SamplesPerSec=0.3497813740529793\n",
            "training loss: 9.63046875\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}