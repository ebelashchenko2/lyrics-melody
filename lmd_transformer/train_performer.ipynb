{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "train_performer.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "41EWXAeL2nYs",
        "outputId": "78d2d8f6-ff40-4d52-bf82-10160503e494"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!pip install drive/MyDrive/lmd_transformer/pytorch_fast_transformers-0.3.0-cp36-cp36m-linux_x86_64.whl\n",
        "!pip install performer-pytorch --upgrade\n",
        "!pip install deepspeed\n",
        "!pip install pytorch-extension\n",
        "!pip install allennlp"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "Processing ./drive/MyDrive/lmd_transformer/pytorch_fast_transformers-0.3.0-cp36-cp36m-linux_x86_64.whl\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from pytorch-fast-transformers==0.3.0) (1.7.0+cu101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch->pytorch-fast-transformers==0.3.0) (1.18.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch->pytorch-fast-transformers==0.3.0) (3.7.4.3)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch->pytorch-fast-transformers==0.3.0) (0.8)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch->pytorch-fast-transformers==0.3.0) (0.16.0)\n",
            "Installing collected packages: pytorch-fast-transformers\n",
            "Successfully installed pytorch-fast-transformers-0.3.0\n",
            "Collecting performer-pytorch\n",
            "  Downloading https://files.pythonhosted.org/packages/0a/12/69ba3d7c66333082efbd4ab55c4616e93838de75d37e3fe95a0d07f0cf6f/performer_pytorch-0.12.7-py3-none-any.whl\n",
            "Collecting local-attention>=1.1.1\n",
            "  Downloading https://files.pythonhosted.org/packages/47/34/21b2a040344a3a785ecee3c268ded02ceb9f8f4a636f20be7729204610a3/local_attention-1.1.1-py3-none-any.whl\n",
            "Collecting einops>=0.3\n",
            "  Downloading https://files.pythonhosted.org/packages/5d/a0/9935e030634bf60ecd572c775f64ace82ceddf2f504a5fd3902438f07090/einops-0.3.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied, skipping upgrade: torch>=1.6 in /usr/local/lib/python3.6/dist-packages (from performer-pytorch) (1.7.0+cu101)\n",
            "Requirement already satisfied, skipping upgrade: pytorch-fast-transformers>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from performer-pytorch) (0.3.0)\n",
            "Requirement already satisfied, skipping upgrade: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.6->performer-pytorch) (0.16.0)\n",
            "Requirement already satisfied, skipping upgrade: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch>=1.6->performer-pytorch) (0.8)\n",
            "Requirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.6/dist-packages (from torch>=1.6->performer-pytorch) (1.18.5)\n",
            "Requirement already satisfied, skipping upgrade: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch>=1.6->performer-pytorch) (3.7.4.3)\n",
            "Installing collected packages: local-attention, einops, performer-pytorch\n",
            "Successfully installed einops-0.3.0 local-attention-1.1.1 performer-pytorch-0.12.7\n",
            "Collecting deepspeed\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d5/f7/e45734d0074336e16f704372c85be7fa867abaa6f5667ffadbadb6ab4fa0/deepspeed-0.3.8.tar.gz (259kB)\n",
            "\u001b[K     |████████████████████████████████| 266kB 16.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=1.2 in /usr/local/lib/python3.6/dist-packages (from deepspeed) (1.7.0+cu101)\n",
            "Requirement already satisfied: torchvision>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from deepspeed) (0.8.1+cu101)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from deepspeed) (4.41.1)\n",
            "Collecting tensorboardX==1.8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c3/12/dcaf67e1312475b26db9e45e7bb6f32b540671a9ee120b3a72d9e09bc517/tensorboardX-1.8-py2.py3-none-any.whl (216kB)\n",
            "\u001b[K     |████████████████████████████████| 225kB 13.5MB/s \n",
            "\u001b[?25hCollecting ninja\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1d/de/393468f2a37fc2c1dc3a06afc37775e27fde2d16845424141d4da62c686d/ninja-1.10.0.post2-py3-none-manylinux1_x86_64.whl (107kB)\n",
            "\u001b[K     |████████████████████████████████| 112kB 33.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch>=1.2->deepspeed) (1.18.5)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch>=1.2->deepspeed) (0.8)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch>=1.2->deepspeed) (3.7.4.3)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.2->deepspeed) (0.16.0)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision>=0.4.0->deepspeed) (7.0.0)\n",
            "Requirement already satisfied: protobuf>=3.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorboardX==1.8->deepspeed) (3.12.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from tensorboardX==1.8->deepspeed) (1.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.2.0->tensorboardX==1.8->deepspeed) (50.3.2)\n",
            "Building wheels for collected packages: deepspeed\n",
            "  Building wheel for deepspeed (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for deepspeed: filename=deepspeed-0.3.8-cp36-none-any.whl size=255693 sha256=13aaeac59ddc72bb02c9f441dd78ebc22afb53cf78d60018d5d2f686b791a8fa\n",
            "  Stored in directory: /root/.cache/pip/wheels/57/8f/5c/22efd14b664ffaebfb35e6d700641e8efab7d19e611242da40\n",
            "Successfully built deepspeed\n",
            "Installing collected packages: tensorboardX, ninja, deepspeed\n",
            "Successfully installed deepspeed-0.3.8 ninja-1.10.0.post2 tensorboardX-1.8\n",
            "Collecting pytorch-extension\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1d/74/2e9110532020880d9ba06085c4b9a163fa8d8993d964bf61aeb217b7896b/pytorch_extension-0.1-py3-none-any.whl (156kB)\n",
            "\u001b[K     |████████████████████████████████| 163kB 14.0MB/s \n",
            "\u001b[?25hInstalling collected packages: pytorch-extension\n",
            "Successfully installed pytorch-extension-0.1\n",
            "Collecting allennlp\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/cb/42/ee4abac910d0becba85ef0c2cfa2d5a954d986a1356708db5a0cd911783c/allennlp-1.2.2-py3-none-any.whl (505kB)\n",
            "\u001b[K     |████████████████████████████████| 512kB 23.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.19 in /usr/local/lib/python3.6/dist-packages (from allennlp) (4.41.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.4.1)\n",
            "Requirement already satisfied: filelock<3.1,>=3.0 in /usr/local/lib/python3.6/dist-packages (from allennlp) (3.0.12)\n",
            "Collecting jsonnet>=0.10.0; sys_platform != \"win32\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/42/40/6f16e5ac994b16fa71c24310f97174ce07d3a97b433275589265c6b94d2b/jsonnet-0.17.0.tar.gz (259kB)\n",
            "\u001b[K     |████████████████████████████████| 266kB 18.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: pytest in /usr/local/lib/python3.6/dist-packages (from allennlp) (3.6.4)\n",
            "Collecting jsonpickle\n",
            "  Downloading https://files.pythonhosted.org/packages/ee/d5/1cc282dc23346a43aab461bf2e8c36593aacd34242bee1a13fa750db0cfe/jsonpickle-1.4.2-py2.py3-none-any.whl\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from allennlp) (0.22.2.post1)\n",
            "Requirement already satisfied: torch<1.8.0,>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.7.0+cu101)\n",
            "Requirement already satisfied: requests>=2.18 in /usr/local/lib/python3.6/dist-packages (from allennlp) (2.23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.18.5)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from allennlp) (2.10.0)\n",
            "Collecting overrides==3.1.0\n",
            "  Downloading https://files.pythonhosted.org/packages/ff/b1/10f69c00947518e6676bbd43e739733048de64b8dd998e9c2d5a71f44c5d/overrides-3.1.0.tar.gz\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from allennlp) (0.8)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (from allennlp) (3.2.5)\n",
            "Requirement already satisfied: tensorboardX>=1.2 in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.8)\n",
            "Requirement already satisfied: spacy<2.4,>=2.1.0 in /usr/local/lib/python3.6/dist-packages (from allennlp) (2.2.4)\n",
            "Collecting boto3<2.0,>=1.14\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2f/f5/aeb4d65266f7712a627674bd19994cee3e1c66ff588adbc4db3fc0bbbf97/boto3-1.16.34-py2.py3-none-any.whl (129kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 57.3MB/s \n",
            "\u001b[?25hCollecting transformers<3.6,>=3.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3a/83/e74092e7f24a08d751aa59b37a9fc572b2e4af3918cb66f7766c3affb1b4/transformers-3.5.1-py3-none-any.whl (1.3MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3MB 56.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (1.9.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (1.15.0)\n",
            "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (0.7.1)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (20.3.0)\n",
            "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (8.6.0)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (1.4.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (50.3.2)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from jsonpickle->allennlp) (3.1.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->allennlp) (0.17.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch<1.8.0,>=1.6.0->allennlp) (3.7.4.3)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch<1.8.0,>=1.6.0->allennlp) (0.16.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18->allennlp) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18->allennlp) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18->allennlp) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18->allennlp) (3.0.4)\n",
            "Requirement already satisfied: protobuf>=3.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorboardX>=1.2->allennlp) (3.12.4)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4,>=2.1.0->allennlp) (1.1.3)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4,>=2.1.0->allennlp) (1.0.5)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4,>=2.1.0->allennlp) (0.4.1)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4,>=2.1.0->allennlp) (7.4.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4,>=2.1.0->allennlp) (3.0.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4,>=2.1.0->allennlp) (0.8.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4,>=2.1.0->allennlp) (1.0.5)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4,>=2.1.0->allennlp) (1.0.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4,>=2.1.0->allennlp) (2.0.5)\n",
            "Collecting s3transfer<0.4.0,>=0.3.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/69/79/e6afb3d8b0b4e96cefbdc690f741d7dd24547ff1f94240c997a26fa908d3/s3transfer-0.3.3-py2.py3-none-any.whl (69kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 11.4MB/s \n",
            "\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1\n",
            "  Downloading https://files.pythonhosted.org/packages/07/cb/5f001272b6faeb23c1c9e0acc04d48eaaf5c862c17709d20e3469c6e0139/jmespath-0.10.0-py2.py3-none-any.whl\n",
            "Collecting botocore<1.20.0,>=1.19.34\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/03/ef/e35e41d6e445f472ac8f4fca8dd22726d8c6dc19ab06317164a222d13599/botocore-1.19.34-py2.py3-none-any.whl (7.0MB)\n",
            "\u001b[K     |████████████████████████████████| 7.1MB 59.4MB/s \n",
            "\u001b[?25hCollecting tokenizers==0.9.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4c/34/b39eb9994bc3c999270b69c9eea40ecc6f0e97991dba28282b9fd32d44ee/tokenizers-0.9.3-cp36-cp36m-manylinux1_x86_64.whl (2.9MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9MB 58.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers<3.6,>=3.4->allennlp) (20.7)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers<3.6,>=3.4->allennlp) (2019.12.20)\n",
            "Collecting sentencepiece==0.1.91\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 48.6MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 53.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->jsonpickle->allennlp) (3.4.0)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.20.0,>=1.19.34->boto3<2.0,>=1.14->allennlp) (2.8.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers<3.6,>=3.4->allennlp) (2.4.7)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers<3.6,>=3.4->allennlp) (7.1.2)\n",
            "Building wheels for collected packages: jsonnet, overrides, sacremoses\n",
            "  Building wheel for jsonnet (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jsonnet: filename=jsonnet-0.17.0-cp36-cp36m-linux_x86_64.whl size=3387923 sha256=b5221b3ee54dbb344d822dc9f4d25382be0fe9d9eed4971a31e7c3fb5912886d\n",
            "  Stored in directory: /root/.cache/pip/wheels/26/7a/37/7dbcc30a6b4efd17b91ad1f0128b7bbf84813bd4e1cfb8c1e3\n",
            "  Building wheel for overrides (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for overrides: filename=overrides-3.1.0-cp36-none-any.whl size=10175 sha256=e2d7b11d7fa10c8f94f0359f631a8da125ace8c8c25c0af4d427e23ea60b9905\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/24/13/6ef8600e6f147c95e595f1289a86a3cc82ed65df57582c65a9\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893261 sha256=022d823baca998c330f727d9a1be05e19206e3cff3c279a4af1e8f6cf954752a\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built jsonnet overrides sacremoses\n",
            "\u001b[31mERROR: botocore 1.19.34 has requirement urllib3<1.27,>=1.25.4; python_version != \"3.4\", but you'll have urllib3 1.24.3 which is incompatible.\u001b[0m\n",
            "Installing collected packages: jsonnet, jsonpickle, overrides, jmespath, botocore, s3transfer, boto3, tokenizers, sentencepiece, sacremoses, transformers, allennlp\n",
            "Successfully installed allennlp-1.2.2 boto3-1.16.34 botocore-1.19.34 jmespath-0.10.0 jsonnet-0.17.0 jsonpickle-1.4.2 overrides-3.1.0 s3transfer-0.3.3 sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.9.3 transformers-3.5.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5GPe_Vj5fUYd",
        "outputId": "0c0d5acf-fca9-4745-d55f-00f295b07e9e"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Thu Dec 10 20:54:55 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 455.45.01    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   50C    P8    10W /  70W |      0MiB / 15079MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XUKjfF4fgQPs",
        "outputId": "2afd0b10-7f2a-4364-e806-9fd5e76cc85b"
      },
      "source": [
        "!ls drive/MyDrive/lmd_transformer"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dataset.parquet\n",
            "instrumental.vocab\n",
            "large\n",
            "pytorch_fast_transformers-0.3.0-cp36-cp36m-linux_x86_64.whl\n",
            "small\n",
            "small_dataset.parquet\n",
            "small_instrumental.vocab\n",
            "small_vocal.vocab\n",
            "vocal.vocab\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ChU7fY2jfdH5",
        "outputId": "241663f1-5c35-4c69-ad4d-f459624e5da7"
      },
      "source": [
        "%%writefile ds_config.json\n",
        "\n",
        "{\n",
        "  \"train_batch_size\": 32,\n",
        "  \"gradient_accumulation_steps\": 8,\n",
        "  \"steps_per_print\": 20,\n",
        "  \"gradient_clipping\": 0.5,\n",
        "  \"optimizer\": {\n",
        "    \"type\": \"Adam\",\n",
        "    \"params\": {\n",
        "      \"lr\": 0.001,\n",
        "      \"betas\": [\n",
        "        0.9,\n",
        "        0.98\n",
        "      ],\n",
        "      \"eps\": 1e-8,\n",
        "      \"weight_decay\" : 0.1\n",
        "    }\n",
        "  },\n",
        "  \"scheduler\": {\n",
        "    \"type\": \"WarmupLR\",\n",
        "    \"params\": {\n",
        "      \"warmup_min_lr\": 0,\n",
        "      \"warmup_max_lr\": 0.001,\n",
        "      \"warmup_num_steps\": 1000\n",
        "    }\n",
        "  }\n",
        "}"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing ds_config.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vvcn21V2FFq5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ac1fdf7-4c66-4d9f-fb13-31f48d5cde4f"
      },
      "source": [
        "%%writefile train_performer.py\n",
        "\n",
        "import deepspeed\n",
        "from performer_pytorch import PerformerEncDec\n",
        "import argparse\n",
        "import random\n",
        "import pandas as pd\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "from allennlp.training.metrics import BLEU\n",
        "from itertools import cycle\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import DataLoader, Dataset, random_split\n",
        "\n",
        "\n",
        "def get_arguments():\n",
        "    parser=argparse.ArgumentParser(description='Lakh Midi Dataset Instruments-Vocals')\n",
        "\n",
        "    parser.add_argument('--dataset-file', '-df', type=str, required=True,\n",
        "                        help='Dataset parquet file')\n",
        "\n",
        "    parser.add_argument('--vocabulary-prefix', '-v', type=str, default='',\n",
        "                        help='Prefix of the vocab files: <pref>_instrumental.vocab, <prf>_vocal.vocab')\n",
        "    \n",
        "    parser.add_argument('--monophonic', '-m', default=False, action='store_true',\n",
        "                        help='Use monophonic instead of full instrumental input')\n",
        "\n",
        "    parser.add_argument('--max-input-sequence-length', '-maxi', type=int, default=-1,\n",
        "                        help='If provided it will skip samples with longer input sequences')\n",
        "    \n",
        "    parser.add_argument('--max-output-sequence-length', '-maxo', type=int, default=-1,\n",
        "                        help='If provided it will skip samples with longer output sequences')\n",
        "    \n",
        "    parser.add_argument('--train-split', '-ts', type=float, default=0.9,\n",
        "                        help='Percentage of the dataset to use for training')\n",
        "\n",
        "    parser.add_argument('--epochs', '-e', type=int, default=20,\n",
        "                        help='Number of epochs')\n",
        "    \n",
        "    parser.add_argument('--validate-every', '-ve', type=int, default=200,\n",
        "                        help='Validate every n batches')\n",
        "    \n",
        "    parser.add_argument('--generate-every', '-ge', type=int, default=500,\n",
        "                        help='Generate every n batches')\n",
        "    \n",
        "    parser.add_argument('--local_rank', type=int, default=-1,\n",
        "                        help='Local rank passed from distributed launcher')\n",
        "    \n",
        "    parser = deepspeed.add_config_arguments(parser)\n",
        "\n",
        "    return parser.parse_args()\n",
        "\n",
        "\n",
        "class MidiDataset(Dataset):\n",
        "    def __init__(self, dataset_file, monophonic, vocabulary_prefix, max_input_length, max_output_length):\n",
        "        super().__init__()\n",
        "        input_type = 'monophonic' if monophonic else 'instrumental'\n",
        "        with open('{}instrumental.vocab'.format(vocabulary_prefix), 'r') as f, \\\n",
        "            open('{}vocal.vocab'.format(vocabulary_prefix), 'r') as g: \n",
        "            self.input_vocab = {w : l for l, w in enumerate(f.read().splitlines())}\n",
        "            self.reverse_input_vocab = {l: w for w, l in self.input_vocab.items()}\n",
        "            self.output_vocab = {w : l for l, w in enumerate(g.read().splitlines())}\n",
        "            self.reverse_output_vocab = {l: w for w, l in self.output_vocab.items()}\n",
        "            \n",
        "        df = pd.read_parquet(dataset_file, columns=['vocal', input_type])\n",
        "        \n",
        "        inp = [self.encode(json.loads(f) + ['<eos>'], is_input=True) for f in df[input_type]]\n",
        "        out = [self.encode(['<bos>'] + json.loads(f) + ['<eos>'], is_input=False) for f in df['vocal']]\n",
        "\n",
        "        if max_input_length < 0 and max_output_length < 0:\n",
        "            self.input = inp\n",
        "            self.output = out\n",
        "        else:\n",
        "            self.input = []\n",
        "            self.output = []\n",
        "            for idx in range(len(inp)):\n",
        "                input_sample = inp[idx]\n",
        "                output_sample = out[idx]\n",
        "                if (max_input_length >= 0 and len(input_sample) > max_input_length) or \\\n",
        "                   (max_output_length >= 0 and len(output_sample) > max_output_length):\n",
        "                   continue\n",
        "                else:\n",
        "                    self.input.append(input_sample)\n",
        "                    self.output.append(output_sample)\n",
        "\n",
        "        self.max_input_length = max([len(f) for f in self.input])\n",
        "        self.max_output_length = max([len(f) for f in self.output])\n",
        "\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return (self.input[index], self.output[index])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input)\n",
        "\n",
        "    def encode(self, event_sequence, is_input):\n",
        "        if is_input:\n",
        "            return torch.tensor([self.input_vocab[i] for i in event_sequence])\n",
        "        else:\n",
        "            return torch.tensor([self.output_vocab[i] for i in event_sequence])\n",
        "\n",
        "    def decode(self, event_sequence, is_input, mask=None):\n",
        "        size = len(event_sequence)\n",
        "        if mask is not None:\n",
        "            mask = mask.tolist()\n",
        "            true_size = len([v for v in mask if v])\n",
        "        else:\n",
        "            true_size = size\n",
        "        if is_input:\n",
        "            return \",\".join([self.reverse_input_vocab[i.item()] for i in event_sequence[:true_size]])\n",
        "        else:\n",
        "            return \",\".join([self.reverse_output_vocab[o.item()] for o in event_sequence[:true_size]])\n",
        "\n",
        "def collate_fn_zero_pad(batch):\n",
        "    inputs, outputs = zip(*batch)\n",
        "    batch_size = len(inputs)\n",
        "\n",
        "    if batch_size == 1:\n",
        "        inputs = inputs[0].view(1, -1)\n",
        "        outputs = outputs[0].view(1, -1)\n",
        "        input_masks = torch.ones_like(inputs).bool()\n",
        "        output_masks = torch.ones_like(outputs).bool()\n",
        "        return (inputs.long(), input_masks), (outputs.long(), output_masks)\n",
        "\n",
        "    input_lengths = [seq.size(0) for seq in inputs]\n",
        "    input_max_length = max(input_lengths)\n",
        "    input_masks = torch.arange(input_max_length).view(1, -1).expand(batch_size, -1) < torch.tensor(input_lengths).view(-1, 1)\n",
        "    padded_inputs = torch.zeros(batch_size, input_max_length)\n",
        "    for i, l in enumerate(input_lengths):\n",
        "        padded_inputs[i, :l] = inputs[i]\n",
        "\n",
        "    output_lengths = [seq.size(0) for seq in outputs]\n",
        "    output_max_length = max(output_lengths)\n",
        "    output_masks = torch.arange(output_max_length).view(1, -1).expand(batch_size, -1) < torch.tensor(output_lengths).view(-1, 1)\n",
        "    padded_outputs = torch.zeros(batch_size, output_max_length)\n",
        "    for i, l in enumerate(output_lengths):\n",
        "        padded_outputs[i, :l] = outputs[i]\n",
        "\n",
        "    return (padded_inputs.long(), input_masks), (padded_outputs.long(), output_masks)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    args = get_arguments()\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    dataset = MidiDataset(dataset_file=args.dataset_file,\n",
        "                          monophonic=args.monophonic,\n",
        "                          vocabulary_prefix=args.vocabulary_prefix,\n",
        "                          max_input_length=args.max_input_sequence_length,\n",
        "                          max_output_length=args.max_output_sequence_length)\n",
        "\n",
        "    train_size = int(args.train_split * len(dataset))\n",
        "    val_size = len(dataset) - train_size\n",
        "    \n",
        "    torch.manual_seed(0)\n",
        "    train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "    writer_train = SummaryWriter(log_dir='drive/MyDrive/lmd_transformer/small/train')\n",
        "    writer_val = SummaryWriter(log_dir='drive/MyDrive/lmd_transformer/small/val')\n",
        "    bleu = BLEU()\n",
        "\n",
        "    model = PerformerEncDec(\n",
        "        dim = 512,\n",
        "        enc_heads = 8,\n",
        "        dec_heads = 8,\n",
        "        enc_depth = 6,\n",
        "        dec_depth = 6,\n",
        "        enc_ff_chunks = 10,\n",
        "        dec_ff_chunks = 10,\n",
        "        enc_num_tokens = len(dataset.input_vocab),\n",
        "        dec_num_tokens = len(dataset.output_vocab),\n",
        "        enc_max_seq_len = dataset.max_input_length,\n",
        "        dec_max_seq_len = dataset.max_output_length,\n",
        "        enc_emb_dropout = 0.1,\n",
        "        dec_emb_dropout = 0.1,\n",
        "        enc_ff_dropout = 0.1,\n",
        "        dec_ff_dropout = 0.1,\n",
        "        enc_attn_dropout = 0.1,\n",
        "        dec_attn_dropout = 0.1,\n",
        "        enc_reversible = True,\n",
        "        dec_reversible = True\n",
        "    ).to(device)\n",
        "\n",
        "    model_engine, optimizer, trainloader, _ = deepspeed.initialize(args=args, model=model, model_parameters=model.parameters(),  training_data=train_dataset, collate_fn=collate_fn_zero_pad)\n",
        "    device = model_engine.local_rank\n",
        "\n",
        "    val_loader_ = DataLoader(val_dataset, batch_size=1, shuffle=True, collate_fn=collate_fn_zero_pad)\n",
        "    val_loader = cycle(val_loader_)\n",
        "\n",
        "\n",
        "    i = None\n",
        "    trainloader = iter(trainloader)\n",
        "    checkpoint_name, client_state = model_engine.load_checkpoint('drive/MyDrive/lmd_transformer/small/')\n",
        "\n",
        "    print(\"\\n\", \"Dataset maximum sequence lengths - Input: {}, Output: {}\".format(dataset.max_input_length, dataset.max_output_length), \"\\n\")\n",
        "    print(\"\\n\", \"Train Dataset - size: {}, batches: {}\".format(len(train_dataset), len(trainloader.dataloader)), \"\\n\")\n",
        "    print(\"\\n\", \"Validate Dataset - size: {}, batches: {}\".format(len(val_dataset), len(val_loader_)), \"\\n\")\n",
        "\n",
        "    if checkpoint_name is not None:\n",
        "        print(\"\\nLoaded checkpoint: {}\\n\".format(checkpoint_name))        \n",
        "        i = client_state['i']\n",
        "        i += 1\n",
        "        epoch, step = divmod(i, len(trainloader.dataloader))\n",
        "        print(\"Epoch: {}, step: {}, i: {}\".format(epoch, step, i))\n",
        "        print(\"Advancing dataloader...\")\n",
        "        for _ in tqdm(range(step)):\n",
        "            next(trainloader)\n",
        "    else:\n",
        "        print(\"\\nNo checkpoint found, training from scratch\\n\")\n",
        "\n",
        "    if i is None:\n",
        "        i = 0\n",
        "        step = 0\n",
        "        epoch = 0\n",
        "\n",
        "    for e in range(args.epochs - epoch):\n",
        "        running_loss = 0\n",
        "        print(\"EPOCH: {}\".format(e + epoch))\n",
        "        while True:\n",
        "            try:\n",
        "                data = next(trainloader)\n",
        "            except StopIteration:\n",
        "                ckpt_id = \"end_of_epoch_{}-{}-{}\".format(e + epoch, i - 1, loss.item())\n",
        "                model_engine.save_checkpoint('drive/MyDrive/lmd_transformer/small/', tag=ckpt_id, client_state = {'i' : i - 1})\n",
        "                step = 0\n",
        "                trainloader = iter(trainloader)\n",
        "                break\n",
        "\n",
        "            model_engine.train()\n",
        "            (inp, inp_mask), (out, out_mask) = data\n",
        "            loss = model_engine(inp.to(device), out.to(device), enc_mask=inp_mask.to(device), dec_mask=out_mask.to(device), return_loss=True)\n",
        "            model_engine.backward(loss)\n",
        "            model_engine.step()\n",
        "            running_loss += loss.item()\n",
        "            if step % 20 == 0:\n",
        "                avg_loss = running_loss / 20 if step > 0 else running_loss\n",
        "                print(\"training loss: {}\".format(avg_loss))\n",
        "                writer_train.add_scalar(\"Loss/train\", avg_loss, i)\n",
        "                writer_train.flush()\n",
        "                running_loss = 0\n",
        "\n",
        "            if step % args.validate_every == 0:\n",
        "                model_engine.eval()\n",
        "                with torch.no_grad():\n",
        "                    running_eval_loss = 0\n",
        "                    for _ in range(40):\n",
        "                        (inp, inp_mask), (out, out_mask) = next(val_loader)\n",
        "                        loss = model_engine(inp.to(device), out.to(device), return_loss=True, enc_mask=inp_mask.to(device), dec_mask=out_mask.to(device))\n",
        "                        running_eval_loss += loss.item()\n",
        "                    print('\\n', f'validation loss: {running_eval_loss / 40}', '\\n')\n",
        "                    writer_val.add_scalar(\"Loss/evaluate\", running_eval_loss / 40, i)\n",
        "                    writer_val.flush()\n",
        "                    running_eval_loss = 0\n",
        "\n",
        "            if step % args.generate_every == 0:\n",
        "                (inp, inp_mask), (expected_out, expected_out_mask) = next(val_loader)\n",
        "                print(dataset.decode(inp[0], is_input=True, mask=inp_mask[0]))\n",
        "                print(dataset.decode(expected_out[0][1:], is_input=False, mask=expected_out_mask[0][1:]))\n",
        "\n",
        "                inp = inp[0].view(1, -1)\n",
        "                inp_mask = inp_mask[0].view(1, -1)\n",
        "                # <bos> token\n",
        "                initial = torch.ones(1,1).long()\n",
        "\n",
        "                out = model_engine.module.generate(inp.to(device), initial.to(device), enc_mask=inp_mask.to(device), seq_len=len(expected_out[0]) - 2, eos_token=2)\n",
        "                print(dataset.decode(out[0], is_input=False))\n",
        "                \n",
        "                bleu(out.to(device), expected_out[:, 1:].to(device))\n",
        "                b = bleu.get_metric(reset=True)['BLEU']\n",
        "                print(\"BLEU metric: {}\".format(b))\n",
        "                writer_val.add_scalar(\"BLEU\", b, i)\n",
        "                writer_val.flush()\n",
        "\n",
        "            if step == 700 :\n",
        "                ckpt_id = \"midepoch_{}-{}-{}\".format(e + epoch, i, loss.item())\n",
        "                model_engine.save_checkpoint('drive/MyDrive/lmd_transformer/small/', tag=ckpt_id, client_state = {'i' : i})\n",
        "            \n",
        "            i += 1\n",
        "            step += 1\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting train_performer.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8eO_LPv6GECN",
        "outputId": "eaa9e2f4-af4f-4dbf-8110-f5203162c0f0"
      },
      "source": [
        "!deepspeed train_performer.py -df drive/MyDrive/lmd_transformer/small_dataset.parquet -v drive/MyDrive/lmd_transformer/small_ --deepspeed --deepspeed_config ds_config.json"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[2020-12-10 21:03:02,674] [WARNING] [runner.py:117:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.\n",
            "[2020-12-10 21:03:02,716] [INFO] [runner.py:355:main] cmd = /usr/bin/python3 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=29500 train_performer.py -df drive/MyDrive/lmd_transformer/small_dataset.parquet -v drive/MyDrive/lmd_transformer/small_ --deepspeed --deepspeed_config ds_config.json\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}